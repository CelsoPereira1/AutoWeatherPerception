{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b2e131-b3de-4150-85ba-62cf69b2ebe2",
   "metadata": {},
   "source": [
    "# Weather and MOR Classification for Autonomous Driving - Multi-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995e4d06-ccd5-4375-b858-6e500a3419de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import io, util\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.color import rgb2hsv, rgb2gray, rgb2yuv\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, io, models\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torchinfo import summary\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import json\n",
    "import seaborn as sns\n",
    "from pyquaternion import Quaternion\n",
    "import fnmatch\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import ordinal_losses_theia as ordinal_losses # ordinal losses from https://github.com/rpmcruz/ordinal-losses\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b963a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging purposes\n",
    "# class Args:\n",
    "#     BATCH=8\n",
    "#     EPOCHS=1\n",
    "#     NETWORK=\"MobileWeatherNet_Early\" # \"MobileNetV3_ViT\", \"ResNet_ViT\", \"MobileNetV3_Early\", and \"RangeWeatherNet_Early\"\n",
    "#     OPT_TECHNIQUE=\"Multi_Adaptive\" # \"Weighted\"\n",
    "#     TRAIN_MODEL=True\n",
    "#     SEED=1998\n",
    "# args=Args()\n",
    "\n",
    "def str_to_bool(value):\n",
    "    if value.lower() in ('yes', 'true', '1'):\n",
    "        return True\n",
    "    elif value.lower() in ('no', 'false', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Invalid value for boolean argument. Accepted values are \"yes\"/\"true\"/\"1\" or \"no\"/\"false\"/\"0\".')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('BATCH', type=int)\n",
    "parser.add_argument('EPOCHS', type=int)\n",
    "parser.add_argument('NETWORK', type=str)\n",
    "parser.add_argument('OPT_TECHNIQUE', type=str)\n",
    "parser.add_argument('TRAIN_MODEL', type=str_to_bool, help='Specify whether to train the model (yes/true/1) or not (no/false/0)')\n",
    "parser.add_argument('SEED', type=int)\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c83600",
   "metadata": {},
   "source": [
    "## System Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30e8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system variables\n",
    "BATCH_SIZE = args.BATCH\n",
    "EPOCHS = args.EPOCHS\n",
    "TRAIN_MODEL = args.TRAIN_MODEL\n",
    "NETWORK = args.NETWORK\n",
    "OPT_TECHNIQUE = args.OPT_TECHNIQUE\n",
    "SEED = args.SEED\n",
    "NUM_TOTAL_SAMPLES = 1293\n",
    "IMAGE_HEIGHT = 1024\n",
    "IMAGE_WIDTH = 1920\n",
    "MEAN = np.array([0.5, 0.5, 0.5])\n",
    "STD = np.array([0.5, 0.5, 0.5])\n",
    "N_CLASSES_WEATHER = 2\n",
    "N_CLASSES_VISIBILITY = 3\n",
    "VISIBILITY_BINS = np.array([40.0, 200.0])\n",
    "# path for saving the models\n",
    "path_best_model = \"./MT_MM_W_MOR_Class_Best_Network_{}_Optimization_{}_Seed_{}.pth\".format(NETWORK, OPT_TECHNIQUE, SEED)\n",
    "path_last_model = \"./MT_MM_W_MOR_Class_Last_Network_{}_Optimization_{}_Seed_{}.pth\".format(NETWORK, OPT_TECHNIQUE, SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d3eee2-6aea-499f-83e1-b82526b406d1",
   "metadata": {},
   "source": [
    "## Transformations / Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0a4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_aug = A.Compose([A.CenterCrop(int(IMAGE_HEIGHT*0.5), int(IMAGE_WIDTH*0.5), p=1.0),\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.Affine(scale=(1.1,1.25), keep_ratio=True, p=0.5),\n",
    "                        A.Normalize(mean=MEAN, std=STD, always_apply=True),\n",
    "                        ToTensorV2()],\n",
    "                        additional_targets={'mask2': 'mask', 'mask3': 'mask'})\n",
    "\n",
    "transform_base = A.Compose([A.CenterCrop(int(IMAGE_HEIGHT*0.5), int(IMAGE_WIDTH*0.5), p=1.0),\n",
    "                            A.Normalize(mean=MEAN, std=STD, always_apply=True),\n",
    "                            ToTensorV2()],\n",
    "                            additional_targets={'mask2': 'mask', 'mask3': 'mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d19d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FogChamber(Dataset):\n",
    "    def load_calib_data(self, path_total_dataset, name_camera_calib, tf_tree, velodyne_name='lidar_hdl64_s3_roof'):\n",
    "        assert velodyne_name in ['lidar_hdl64_s3_roof', 'lidar_vlp32_roof'], 'wrong frame id in tf_tree for velodyne_name'\n",
    "\n",
    "        with open(os.path.join(path_total_dataset, name_camera_calib), 'r') as f:\n",
    "            data_camera = json.load(f)\n",
    "        with open(os.path.join(path_total_dataset, tf_tree), 'r') as f:\n",
    "            data_extrinsics = json.load(f)\n",
    "\n",
    "        calib_dict = {'calib_cam_stereo_left.json': 'cam_stereo_left_optical',\n",
    "                    'calib_cam_stereo_right.json': 'cam_stereo_right_optical',\n",
    "                    'calib_gated_bwv.json': 'bwv_cam_optical'}\n",
    "\n",
    "        cam_name = calib_dict[name_camera_calib]\n",
    "\n",
    "        # scan data extrinsics for transformation from lidar to camera\n",
    "        important_translations = [velodyne_name, 'radar', cam_name]\n",
    "        translations = []\n",
    "\n",
    "        for item in data_extrinsics:\n",
    "            if item['child_frame_id'] in important_translations:\n",
    "                translations.append(item)\n",
    "                if item['child_frame_id'] == cam_name:\n",
    "                    T_cam = item['transform']\n",
    "                elif item['child_frame_id'] == velodyne_name:\n",
    "                    T_velodyne = item['transform']\n",
    "                elif item['child_frame_id'] == 'radar':\n",
    "                    T_radar = item['transform']\n",
    "\n",
    "        # use pyquaternion to setup rotation matrices properly\n",
    "        R_c_quaternion = Quaternion(w=T_cam['rotation']['w'] * 360 / 2 / np.pi, x=T_cam['rotation']['x'] * 360 / 2 / np.pi,\n",
    "                                    y=T_cam['rotation']['y'] * 360 / 2 / np.pi, z=T_cam['rotation']['z'] * 360 / 2 / np.pi)\n",
    "        R_v_quaternion = Quaternion(w=T_velodyne['rotation']['w'] * 360 / 2 / np.pi,\n",
    "                                    x=T_velodyne['rotation']['x'] * 360 / 2 / np.pi,\n",
    "                                    y=T_velodyne['rotation']['y'] * 360 / 2 / np.pi,\n",
    "                                    z=T_velodyne['rotation']['z'] * 360 / 2 / np.pi)\n",
    "\n",
    "        # setup quaternion values as 3x3 orthogonal rotation matrices\n",
    "        R_c_matrix = R_c_quaternion.rotation_matrix\n",
    "        R_v_matrix = R_v_quaternion.rotation_matrix\n",
    "\n",
    "        # setup translation Vectors\n",
    "        Tr_cam = np.asarray([T_cam['translation']['x'], T_cam['translation']['y'], T_cam['translation']['z']])\n",
    "        Tr_velodyne = np.asarray([T_velodyne['translation']['x'], T_velodyne['translation']['y'], T_velodyne['translation']['z']])\n",
    "\n",
    "        # setup Translation Matrix camera to lidar -> ROS spans transformation from its children to its parents therefore one inversion step is needed for zero_to_camera -> <parent_child>\n",
    "        zero_to_camera = np.zeros((3, 4))\n",
    "        zero_to_camera[0:3, 0:3] = R_c_matrix\n",
    "        zero_to_camera[0:3, 3] = Tr_cam\n",
    "        zero_to_camera = np.vstack((zero_to_camera, np.array([0, 0, 0, 1])))\n",
    "\n",
    "        zero_to_velodyne = np.zeros((3, 4))\n",
    "        zero_to_velodyne[0:3, 0:3] = R_v_matrix\n",
    "        zero_to_velodyne[0:3, 3] = Tr_velodyne\n",
    "        zero_to_velodyne = np.vstack((zero_to_velodyne, np.array([0, 0, 0, 1])))\n",
    "\n",
    "        # calculate total extrinsic transformation to camera\n",
    "        velodyne_to_camera = np.matmul(np.linalg.inv(zero_to_camera), zero_to_velodyne)\n",
    "        camera_to_velodyne = np.matmul(np.linalg.inv(zero_to_velodyne), zero_to_camera)\n",
    "\n",
    "        # read projection matrix P and camera rectification matrix R\n",
    "        P = np.reshape(data_camera['P'], [3, 4])\n",
    "\n",
    "        # rectification matrix R has to be equal to the identity as the projection matrix P contains the R matrix w.r.t KITTI definition\n",
    "        R = np.identity(4)\n",
    "\n",
    "        # calculate total transformation matrix from velodyne to camera\n",
    "        vtc = np.matmul(np.matmul(P, R), velodyne_to_camera)\n",
    "        return velodyne_to_camera, camera_to_velodyne, P, R, vtc, zero_to_camera\n",
    "\n",
    "    def py_func_project_3D_to_2D(self, points_3D, P):\n",
    "        # project on image\n",
    "        points_2D = np.matmul(P, np.vstack((points_3D, np.ones([1, np.shape(points_3D)[1]]))))\n",
    "\n",
    "        # scale projected points\n",
    "        points_2D[0][:] = points_2D[0][:] / points_2D[2][:]\n",
    "        points_2D[1][:] = points_2D[1][:] / points_2D[2][:]\n",
    "\n",
    "        points_2D = points_2D[0:2]\n",
    "        return points_2D.transpose()\n",
    "\n",
    "    def weather_digitize(self, weather_data):\n",
    "        weather_in_bins = np.array([])\n",
    "        if weather_data == 'Fog Small Droplets':\n",
    "            weather_in_bins = 0\n",
    "        elif weather_data == 'Rain':\n",
    "            weather_in_bins = 1\n",
    "        else:\n",
    "            assert False, \"Invalid weather class.\"\n",
    "        return weather_in_bins\n",
    "\n",
    "    def files_with_equal_names_in_diff_dir(self, dir1, dir2, dir3, ext1, ext2, ext3):\n",
    "        files1 = os.listdir(dir1)\n",
    "        files2 = os.listdir(dir2)\n",
    "        files3 = os.listdir(dir3)\n",
    "        matches = []\n",
    "\n",
    "        for file1 in files1:\n",
    "            if fnmatch.fnmatch(file1, '*' + ext1):\n",
    "                name1 = os.path.splitext(file1)[0]\n",
    "                for file2 in files2:\n",
    "                    if fnmatch.fnmatch(file2, name1 + ext2):\n",
    "                        for file3 in files3:\n",
    "                            if fnmatch.fnmatch(file3, name1 + ext3):\n",
    "                                matches.append((os.path.join(dir1, file1), os.path.join(dir2, file2), os.path.join(dir3, file3)))\n",
    "        return matches\n",
    "    \n",
    "    def __init__(self, root, subset, rgb_fold, lidar_fold, weather_visibility_fold, dict_transform=None):\n",
    "        _, _, _, _, self.vtc,_ = self.load_calib_data(root, 'calib_cam_stereo_left.json', 'calib_tf_tree_full.json', 'lidar_hdl64_s3_roof')\n",
    "        self.subset = subset\n",
    "\n",
    "        assert rgb_fold in ('cam_stereo_left_lut')\n",
    "        self.rgb_path = os.path.join(root, rgb_fold)\n",
    "        self.rgb_extension = '.png'\n",
    "\n",
    "        assert lidar_fold in ('lidar_hdl64_strongest', 'lidar_hdl64_last')\n",
    "        self.lidar_path = os.path.join(root, lidar_fold)\n",
    "        self.lidar_extension = '.bin'\n",
    "\n",
    "        assert weather_visibility_fold in ('cerema')\n",
    "        self.weather_visibility_path = os.path.join(root, weather_visibility_fold)\n",
    "        self.weather_visibility_extension = '.json'\n",
    "\n",
    "        self.file_matches = sorted(self.files_with_equal_names_in_diff_dir(self.rgb_path, self.lidar_path, self.weather_visibility_path, self.rgb_extension, self.lidar_extension, self.weather_visibility_extension))\n",
    "        self.rgb_files = [i[0] for i in self.file_matches]\n",
    "        self.lidar_files = [i[1] for i in self.file_matches]\n",
    "        self.weather_visibility_files = [i[2] for i in self.file_matches]\n",
    "\n",
    "        self.dict_transform = dict_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i, only_weather=False, only_visibility=False):\n",
    "        j = self.subset[i]\n",
    "\n",
    "        if only_weather:\n",
    "            # get weather GT\n",
    "            weather_visibility_fname = self.weather_visibility_files[j]\n",
    "            weather_data = json.load(open(weather_visibility_fname))['weather']\n",
    "            weather_in_bins = self.weather_digitize(weather_data)\n",
    "            return weather_in_bins\n",
    "        \n",
    "        if only_visibility:\n",
    "            # get visibility GT\n",
    "            weather_visibility_fname = self.weather_visibility_files[j]\n",
    "            visibility_data = json.load(open(weather_visibility_fname))['metereological_visibility']\n",
    "            visibility_in_bins = np.digitize(visibility_data, VISIBILITY_BINS)\n",
    "            return visibility_in_bins\n",
    "    \n",
    "        # get LUT RGB image\n",
    "        rgb_fname = self.rgb_files[j]\n",
    "        rgb_image = imread(rgb_fname)\n",
    "\n",
    "        # image entropy (normalised)\n",
    "        gray = rgb2gray(rgb_image)\n",
    "        image_entropy = entropy(util.img_as_ubyte(gray), disk(5)) / 6.1917 # 6.1917 is the max image entropy for disk 5\n",
    "\n",
    "        # load and parse lidar data\n",
    "        lidar_fname = self.lidar_files[j]\n",
    "        lidar_data_raw = np.fromfile(lidar_fname, dtype=np.float32).reshape((-1, 5))\n",
    "        # filter away all points behind image plane and below distance threshold\n",
    "        r = np.sqrt(lidar_data_raw[:, 0] ** 2 + lidar_data_raw[:, 1] ** 2 + lidar_data_raw[:, 2] ** 2)\n",
    "        lidar_data_raw = lidar_data_raw[np.where(r > 1.5)]\n",
    "        lidar_data_raw = lidar_data_raw[np.where(lidar_data_raw[:, 0] > 2.5)]\n",
    "        # range calculation\n",
    "        lidar_range = np.sqrt(lidar_data_raw[:, 0] ** 2 + lidar_data_raw[:, 1] ** 2 + lidar_data_raw[:, 2] ** 2)\n",
    "        # 3D to 2D valid coordinates\n",
    "        points_2D = self.py_func_project_3D_to_2D(lidar_data_raw[:, 0:3].transpose(), self.vtc)\n",
    "        within_image_boarder_width = np.logical_and(IMAGE_WIDTH > points_2D[:, 0], points_2D[:, 0] >= 0)                   \n",
    "        within_image_boarder_height = np.logical_and(IMAGE_HEIGHT > points_2D[:, 1], points_2D[:, 1] >= 0)\n",
    "        valid_points = np.logical_and(within_image_boarder_width, within_image_boarder_height)\n",
    "        coordinates = np.where(valid_points)[0]\n",
    "        img_coordinates = points_2D[coordinates, :].astype(dtype=np.int32)\n",
    "        # lidar range image (normalised)\n",
    "        image_lidar_range = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image_lidar_range[img_coordinates[:, 0], img_coordinates[:, 1]] = lidar_range[coordinates] / 109.2780 # 109.2780 is the max range\n",
    "        image_lidar_range = image_lidar_range.transpose()\n",
    "        # lidar intensity image (normalised)\n",
    "        image_lidar_intensity = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image_lidar_intensity[img_coordinates[:, 0], img_coordinates[:, 1]] = lidar_data_raw[:,3][coordinates] / 255 # 255 is the max intensity\n",
    "        image_lidar_intensity = image_lidar_intensity.transpose()\n",
    "\n",
    "        # get weather and visibility GT\n",
    "        weather_visibility_fname = self.weather_visibility_files[j]\n",
    "        weather_data = json.load(open(weather_visibility_fname))['weather']\n",
    "        weather_in_bins = self.weather_digitize(weather_data)\n",
    "        visibility_data = json.load(open(weather_visibility_fname))['metereological_visibility']\n",
    "        visibility_in_bins = np.digitize(visibility_data, VISIBILITY_BINS)\n",
    "\n",
    "        if self.dict_transform:\n",
    "            transformed = self.dict_transform(image=rgb_image, mask=image_lidar_range, mask2=image_lidar_intensity, mask3=image_entropy)\n",
    "            image_lidar_range = transformed['mask']\n",
    "            image_lidar_intensity = transformed['mask2']\n",
    "            image_entropy = transformed['mask3']\n",
    "        \n",
    "        return weather_in_bins, visibility_in_bins, image_entropy, image_lidar_range, image_lidar_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22aa7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[704  71]\n"
     ]
    }
   ],
   "source": [
    "# to ensure reproducibility\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "\n",
    "num_tr_samples = int(0.6 * NUM_TOTAL_SAMPLES)\n",
    "num_vl_samples = int(0.2 * NUM_TOTAL_SAMPLES)\n",
    "num_ts_samples = int(0.2 * NUM_TOTAL_SAMPLES)\n",
    "data = list(range(0, NUM_TOTAL_SAMPLES))\n",
    "random.shuffle(data) # does NOT prevent leakage between sequences, but NO better solution has been found\n",
    "\n",
    "tr_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[:num_tr_samples], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_aug)\n",
    "vl_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[num_tr_samples:num_tr_samples + num_vl_samples], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_base)\n",
    "ts_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[num_tr_samples + num_vl_samples:], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_base)\n",
    "\n",
    "# manage class imbalance\n",
    "weather_samples = [tr_dataset.__getitem__(i, only_weather=True) for i in range(len(tr_dataset))]\n",
    "_, weather_class_sample_count = np.unique(weather_samples, return_counts=True)\n",
    "print(weather_class_sample_count)\n",
    "weather_class_weights = 1 / torch.tensor(weather_class_sample_count)\n",
    "weight_per_sample = weather_class_weights[weather_samples]\n",
    "weather_sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_per_sample, len(weight_per_sample))\n",
    "\n",
    "tr = DataLoader(tr_dataset, BATCH_SIZE, sampler=weather_sampler, num_workers=16, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "vl = DataLoader(vl_dataset, BATCH_SIZE, num_workers=16, shuffle=False, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "ts = DataLoader(ts_dataset, BATCH_SIZE, num_workers=16, shuffle=False, pin_memory=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7193333e-5b64-42d0-ac1a-79956966d62f",
   "metadata": {},
   "source": [
    "## Network and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdde5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RangeWeatherNet Custom - Multi-Task\n",
    "def conv_batch_range(in_num, out_num, kernel_size=3, padding=1, stride=1, dropout=0.05):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_num, out_num, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_num),\n",
    "        nn.LeakyReLU(0.4),\n",
    "        nn.Dropout(p=dropout))\n",
    "\n",
    "# residual block\n",
    "class DarkResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        reduced_channels = int(in_channels/2)\n",
    "        self.layer1 = conv_batch_range(in_channels, reduced_channels, kernel_size=1, padding=0)\n",
    "        self.layer2 = conv_batch_range(reduced_channels, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class Range_Weather_Net_Custom(nn.Module):\n",
    "    def make_layer(self, block, in_channels, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(0, num_blocks):\n",
    "            layers.append(block(in_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, block, input_channels, num_classes_weather, num_classes_visibility, perception_output_features=512):\n",
    "        super().__init__()\n",
    "\n",
    "        conv1 = conv_batch_range(input_channels, 16)\n",
    "        conv2 = conv_batch_range(16, 32, stride=2)\n",
    "        residual_block1 = self.make_layer(block, in_channels=32, num_blocks=1)\n",
    "        conv3 = conv_batch_range(32, 64, stride=2)\n",
    "        residual_block2 = self.make_layer(block, in_channels=64, num_blocks=2)\n",
    "        conv4 = conv_batch_range(64, 128, stride=2)\n",
    "        residual_block3 = self.make_layer(block, in_channels=128, num_blocks=8)\n",
    "        conv5 = conv_batch_range(128, 256, stride=2)\n",
    "        residual_block4 = self.make_layer(block, in_channels=256, num_blocks=8)\n",
    "        conv6 = nn.Conv2d(256, perception_output_features, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "        self.backbone = nn.Sequential(conv1, conv2,\n",
    "                                      residual_block1, conv3,\n",
    "                                      residual_block2, conv4,\n",
    "                                      residual_block3, conv5,\n",
    "                                      residual_block4, conv6)\n",
    "\n",
    "        # head for weather\n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "        # head for visibility\n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # apply global average pooling to collapse spatial dimensions\n",
    "        out = nn.AdaptiveAvgPool2d(1)(out)\n",
    "        # heads for weather and visibility\n",
    "        out_weather = self.head_weather(out)\n",
    "        out_visibility = self.head_visibility(out)\n",
    "        # squeeze the output to remove singleton dimensions\n",
    "        out_weather = out_weather.squeeze(2).squeeze(2)\n",
    "        out_visibility = out_visibility.squeeze(2).squeeze(2)\n",
    "        return out_weather, out_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746a6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileWeatherNet Custom - Multi-Task\n",
    "class Mobile_Weather_Net_Custom(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes_weather, num_classes_visibility, perception_output_features=512):\n",
    "        super().__init__()\n",
    "\n",
    "        block_1 = nn.Sequential(self.conv_batch(in_num=input_channels, out_num=32, stride=2),\n",
    "                                self.conv_batch(in_num=32, out_num=32))\n",
    "        \n",
    "        block_2 = nn.Sequential(self.conv_batch(in_num=32, out_num=64, stride=2),\n",
    "                                self.conv_batch(in_num=64, out_num=64))\n",
    "        \n",
    "        block_3 = nn.Sequential(self.conv_batch(in_num=64, out_num=128, stride=2),\n",
    "                                self.conv_batch(in_num=128, out_num=128),\n",
    "                                self.conv_batch(in_num=128, out_num=128))\n",
    "        \n",
    "        conv = nn.Conv2d(128, perception_output_features, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.backbone = nn.Sequential(block_1, block_2, block_3, conv)\n",
    "\n",
    "        # head for weather\n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "        # head for visibility\n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # apply global average pooling to collapse spatial dimensions\n",
    "        out = nn.AdaptiveAvgPool2d(1)(out)\n",
    "        # heads for weather and visibility\n",
    "        out_weather = self.head_weather(out)\n",
    "        out_visibility = self.head_visibility(out)\n",
    "        # squeeze the output to remove singleton dimensions\n",
    "        out_weather = out_weather.squeeze(2).squeeze(2)\n",
    "        out_visibility = out_visibility.squeeze(2).squeeze(2)\n",
    "        return out_weather, out_visibility\n",
    "    \n",
    "    def conv_batch(self, in_num, out_num, kernel_size=3, stride=1, padding=1, dropout=0.05):\n",
    "        return nn.Sequential(nn.Conv2d(in_num, out_num, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "                            nn.BatchNorm2d(out_num),\n",
    "                            nn.PReLU(),\n",
    "                            nn.Dropout(p=dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV3 Custom - Multi-Task\n",
    "class Mobile_Net_v3_Custom(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes_weather, num_classes_visibility, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "\n",
    "        self.backbone.features[0][0] = nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.backbone.avgpool = nn.Sequential(nn.Conv2d(self.backbone.features[12][0].out_channels, perception_output_features, (1, 1)),\n",
    "                                              nn.AdaptiveAvgPool2d(output_size=1))\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # reshape the backbone output to (batch_size, n_features, 1, 1)\n",
    "        out = out.view(out.size(0), -1, 1, 1)\n",
    "        # forward pass through the heads\n",
    "        out_weather = self.head_weather(out).view(out.size(0), -1)\n",
    "        out_visibility = self.head_visibility(out).view(out.size(0), -1)\n",
    "        return out_weather, out_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fdebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head masked self-attention layer with a projection at the end\n",
    "# code based on the article \"Multi-Modal Fusion Transformer for End-to-End Autonomous Driving\", Aditya Prakash, Kashyap Chitta, Andreas Geiger, 2021\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerBlock(nn.Module): # transformer block\n",
    "    def __init__(self, n_embd, n_head, block_exp, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MSA(n_embd, n_head, attn_pdrop, resid_pdrop)\n",
    "        self.mlp = nn.Sequential(nn.Linear(n_embd, block_exp * n_embd),\n",
    "                                nn.ReLU(True), # changed from GELU\n",
    "                                nn.Linear(block_exp * n_embd, n_embd),\n",
    "                                nn.Dropout(resid_pdrop))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_exp, n_layer, camera_vert_anchors, camera_horz_anchors, lidar_vert_anchors, lidar_horz_anchors, seq_len, embd_pdrop, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len # only support seq len 1\n",
    "        self.camera_vert_anchors = camera_vert_anchors\n",
    "        self.camera_horz_anchors = camera_horz_anchors\n",
    "        self.lidar_vert_anchors = lidar_vert_anchors\n",
    "        self.lidar_horz_anchors = lidar_horz_anchors\n",
    "        # positional embedding parameter (learnable), camera + lidar\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.seq_len * camera_vert_anchors * camera_horz_anchors + self.seq_len * lidar_vert_anchors * lidar_horz_anchors, n_embd))\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, block_exp, attn_pdrop, resid_pdrop) for layer in range(n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.block_size = self.seq_len\n",
    "\n",
    "    def forward(self, camera_tensor, lidar_tensor):\n",
    "        bz = lidar_tensor.shape[0]\n",
    "        lidar_h, lidar_w = lidar_tensor.shape[2:4]\n",
    "        camera_h, camera_w = camera_tensor.shape[2:4]\n",
    "\n",
    "        assert self.seq_len == 1\n",
    "        camera_tensor = camera_tensor.view(bz, self.seq_len, -1, camera_h, camera_w).permute(0,1,3,4,2).contiguous().view(bz, -1, self.n_embd)\n",
    "        lidar_tensor = lidar_tensor.view(bz, self.seq_len, -1, lidar_h, lidar_w).permute(0,1,3,4,2).contiguous().view(bz, -1, self.n_embd)\n",
    "        token_embeddings = torch.cat((camera_tensor, lidar_tensor), dim=1)\n",
    "\n",
    "        x = self.drop(self.pos_emb + token_embeddings)\n",
    "        x = self.blocks(x) # (B, an * T, C)\n",
    "        x = self.ln_f(x) # (B, an * T, C)\n",
    "        x = x.view(bz, self.seq_len * self.camera_vert_anchors * self.camera_horz_anchors + self.seq_len * self.lidar_vert_anchors * self.lidar_horz_anchors, self.n_embd)\n",
    "\n",
    "        camera_tensor_out = x[:, :self.seq_len*self.camera_vert_anchors*self.camera_horz_anchors, :].contiguous().view(bz * self.seq_len, -1, camera_h, camera_w)\n",
    "        lidar_tensor_out = x[:, self.seq_len*self.camera_vert_anchors*self.camera_horz_anchors:, :].contiguous().view(bz * self.seq_len, -1, lidar_h, lidar_w)\n",
    "\n",
    "        return camera_tensor_out, lidar_tensor_out\n",
    "    \n",
    "class MobileNetV3_Encoder(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_camera.features[0][0] = nn.Conv2d(camera_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.camera_n_features = self.backbone_camera.classifier[0].in_features\n",
    "        self.backbone_camera.classifier = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_lidar.features[0][0] = nn.Conv2d(lidar_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.lidar_n_features = self.backbone_lidar.classifier[0].in_features\n",
    "        self.backbone_lidar.classifier = nn.Identity()\n",
    "    \n",
    "class ResNet_Encoder(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera = torchvision.models.resnet34(weights=None)\n",
    "        self.backbone_camera.conv1 = nn.Conv2d(camera_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.camera_n_features = self.backbone_camera.fc.in_features\n",
    "        self.backbone_camera.fc = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar = torchvision.models.resnet18(weights=None)\n",
    "        self.backbone_lidar.conv1 = nn.Conv2d(lidar_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.lidar_n_features = self.backbone_lidar.fc.in_features\n",
    "        self.backbone_lidar.fc = nn.Identity()\n",
    "\n",
    "class ViT_FusionNet(nn.Module):\n",
    "    def __init__(self, num_classes_weather, num_classes_visibility, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=4, block_exp=4, n_layer=8, seq_len=1, embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.architecture = architecture\n",
    "\n",
    "        camera_vert_anchors = 16\n",
    "        camera_horz_anchors = 30\n",
    "        lidar_vert_anchors = 16\n",
    "        lidar_horz_anchors = 30\n",
    "\n",
    "        self.avgpool_camera = nn.AdaptiveAvgPool2d((camera_vert_anchors, camera_horz_anchors))\n",
    "        self.avgpool_lidar = nn.AdaptiveAvgPool2d((lidar_vert_anchors, lidar_horz_anchors))\n",
    "\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            self.encoders = ResNet_Encoder(camera_in_channels, lidar_in_channels)\n",
    "\n",
    "            # n_embd (int) - number of expected features in the encoder/decoder inputs, n_head (int) - number of heads in the multiheadattention models\n",
    "            self.transformer1 = GPT(n_embd=self.encoders.backbone_camera.layer1[1].conv2.out_channels, # note: in_channels can also be accessed\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2 = GPT(n_embd=self.encoders.backbone_camera.layer2[1].conv2.out_channels,\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer3 = GPT(n_embd=self.encoders.backbone_camera.layer3[1].conv2.out_channels,\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer4 = GPT(n_embd=self.encoders.backbone_camera.layer4[1].conv2.out_channels,\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            if self.encoders.backbone_camera.layer4[1].conv2.out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera = nn.Conv2d(self.encoders.backbone_camera.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar = nn.Conv2d(self.encoders.backbone_camera.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera = nn.Sequential()\n",
    "                self.change_channel_conv_lidar = nn.Sequential()\n",
    "\n",
    "        elif self.architecture == \"MobileNetV3_ViT\":\n",
    "            self.encoders = MobileNetV3_Encoder(camera_in_channels, lidar_in_channels)\n",
    "\n",
    "            self.transformer1 = GPT(n_embd=self.encoders.backbone_camera.features[6].block[3][0].out_channels,\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2 = GPT(n_embd=self.encoders.backbone_camera.features[12][0].out_channels,\n",
    "                                n_head=n_head,\n",
    "                                block_exp=block_exp,\n",
    "                                n_layer=n_layer,\n",
    "                                camera_vert_anchors=camera_vert_anchors,\n",
    "                                camera_horz_anchors=camera_horz_anchors,\n",
    "                                lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                seq_len=seq_len,\n",
    "                                embd_pdrop=embd_pdrop,\n",
    "                                attn_pdrop=attn_pdrop,\n",
    "                                resid_pdrop=resid_pdrop)\n",
    "\n",
    "            if self.encoders.backbone_camera.features[12][0].out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera = nn.Conv2d(self.encoders.backbone_camera.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar = nn.Conv2d(self.encoders.backbone_camera.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera = nn.Sequential()\n",
    "                self.change_channel_conv_lidar = nn.Sequential()\n",
    "                \n",
    "        else:\n",
    "            assert False, \"Invalid architecture.\"\n",
    "\n",
    "        # classification heads\n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, in_camera, in_lidar):\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            camera_features = self.encoders.backbone_camera.conv1(in_camera)\n",
    "            camera_features = self.encoders.backbone_camera.bn1(camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.relu(camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.maxpool(camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.conv1(in_lidar)\n",
    "            lidar_features = self.encoders.backbone_lidar.bn1(lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.relu(lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.maxpool(lidar_features)\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.layer1(camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.layer1(lidar_features)\n",
    "            camera_features_layer1 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_layer1 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_layer1, lidar_features_layer1 = self.transformer1(camera_features_layer1, lidar_features_layer1)\n",
    "            camera_features_layer1 = F.interpolate(camera_features_layer1, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer1 = F.interpolate(lidar_features_layer1, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_layer1\n",
    "            lidar_features = lidar_features + lidar_features_layer1\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.layer2(camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.layer2(lidar_features)\n",
    "            camera_features_layer2 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_layer2 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_layer2, lidar_features_layer2 = self.transformer2(camera_features_layer2, lidar_features_layer2)\n",
    "            camera_features_layer2 = F.interpolate(camera_features_layer2, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer2 = F.interpolate(lidar_features_layer2, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_layer2\n",
    "            lidar_features = lidar_features + lidar_features_layer2\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.layer3(camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.layer3(lidar_features)\n",
    "            camera_features_layer3 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_layer3 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_layer3, lidar_features_layer3 = self.transformer3(camera_features_layer3, lidar_features_layer3)\n",
    "            camera_features_layer3 = F.interpolate(camera_features_layer3, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer3 = F.interpolate(lidar_features_layer3, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_layer3\n",
    "            lidar_features = lidar_features + lidar_features_layer3\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.layer4(camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.layer4(lidar_features)\n",
    "            camera_features_layer4 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_layer4 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_layer4, lidar_features_layer4 = self.transformer4(camera_features_layer4, lidar_features_layer4)\n",
    "            camera_features_layer4 = F.interpolate(camera_features_layer4, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer4 = F.interpolate(lidar_features_layer4, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_layer4\n",
    "            lidar_features = lidar_features + lidar_features_layer4\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features = self.change_channel_conv_camera(camera_features)\n",
    "            lidar_features = self.change_channel_conv_lidar(lidar_features)\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.avgpool(camera_features)\n",
    "            camera_features = torch.flatten(camera_features, 1)\n",
    "            lidar_features = self.encoders.backbone_lidar.avgpool(lidar_features)\n",
    "            lidar_features = torch.flatten(lidar_features, 1)\n",
    "            fused_features = camera_features + lidar_features\n",
    "        \n",
    "        else:\n",
    "            camera_features = self.encoders.backbone_camera.features[0](in_camera)\n",
    "            camera_features = self.encoders.backbone_camera.features[1](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[2](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[3](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[4](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[5](camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[0](in_lidar)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[1](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[2](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[3](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[4](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[5](lidar_features)\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.features[6](camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[6](lidar_features)\n",
    "            camera_features_block1 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_block1 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_block1, lidar_features_block1 = self.transformer1(camera_features_block1, lidar_features_block1)\n",
    "            camera_features_block1 = F.interpolate(camera_features_block1, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block1 = F.interpolate(lidar_features_block1, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_block1\n",
    "            lidar_features = lidar_features + lidar_features_block1\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.features[7](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[8](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[9](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[10](camera_features)\n",
    "            camera_features = self.encoders.backbone_camera.features[11](camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[7](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[8](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[9](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[10](lidar_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[11](lidar_features)\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.features[12](camera_features)\n",
    "            lidar_features = self.encoders.backbone_lidar.features[12](lidar_features)\n",
    "            camera_features_block2 = self.avgpool_camera(camera_features)\n",
    "            lidar_features_block2 = self.avgpool_lidar(lidar_features)\n",
    "            camera_features_block2, lidar_features_block2 = self.transformer2(camera_features_block2, lidar_features_block2)\n",
    "            camera_features_block2 = F.interpolate(camera_features_block2, size=(camera_features.shape[2], camera_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block2 = F.interpolate(lidar_features_block2, size=(lidar_features.shape[2], lidar_features.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features = camera_features + camera_features_block2\n",
    "            lidar_features = lidar_features + lidar_features_block2\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features = self.change_channel_conv_camera(camera_features)\n",
    "            lidar_features = self.change_channel_conv_lidar(lidar_features)\n",
    "\n",
    "            camera_features = self.encoders.backbone_camera.avgpool(camera_features)\n",
    "            camera_features = torch.flatten(camera_features, 1)\n",
    "            lidar_features = self.encoders.backbone_lidar.avgpool(lidar_features)\n",
    "            lidar_features = torch.flatten(lidar_features, 1)\n",
    "            fused_features = camera_features + lidar_features\n",
    "\n",
    "        # reshape the output to (batch_size, n_features, 1, 1)\n",
    "        fused_features = fused_features.view(fused_features.size(0), -1, 1, 1)\n",
    "        # forward pass through the heads\n",
    "        out_weather = self.head_weather(fused_features).view(fused_features.size(0), -1)\n",
    "        out_visibility = self.head_visibility(fused_features).view(fused_features.size(0), -1)\n",
    "        \n",
    "        return out_weather, out_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8e29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # useful when there is a large class imbalance. alpha (Tensor, optional): weights for each class. Defaults to None. gamma (float, optional): a constant. Defaults to 0.\n",
    "    # reduction (str, optional): 'mean', 'sum', or 'none', ignore_index (int, optional): class label to ignore. Defaults to -100\n",
    "    def __init__(self, alpha=None, gamma=0.0, reduction='mean', ignore_index=-100):\n",
    "        super().__init__()\n",
    "\n",
    "        assert reduction in ('mean', 'sum', 'none')\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eps = 0.001 # avoid grad explode\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=self.alpha, reduction='none', ignore_index=self.ignore_index)\n",
    "\n",
    "    def forward(self, pred_prob, target):\n",
    "        if pred_prob.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dk) -> (N * d1 * ... * dk, C)\n",
    "            c = pred_prob.shape[1]\n",
    "            pred_prob = pred_prob.permute(0, *range(2, pred_prob.ndim), 1).reshape(-1, c)\n",
    "            target = target.view(-1)\n",
    "\n",
    "        unignored_mask = target != self.ignore_index\n",
    "        target = target[unignored_mask]\n",
    "        if len(target) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        pred_prob = pred_prob[unignored_mask]\n",
    "        \n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = torch.log(pred_prob + self.eps)\n",
    "        ce = self.nll_loss(log_p, target)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(pred_prob))\n",
    "        log_pt = log_p[all_rows, target]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027c9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Mobile_Weather_Net_Custom                --\n",
      "Sequential: 1-1                        --\n",
      "    Sequential: 2-1                   --\n",
      "        Sequential: 3-1              929\n",
      "        Sequential: 3-2              9,281\n",
      "    Sequential: 2-2                   --\n",
      "        Sequential: 3-3              18,561\n",
      "        Sequential: 3-4              36,993\n",
      "    Sequential: 2-3                   --\n",
      "        Sequential: 3-5              73,985\n",
      "        Sequential: 3-6              147,713\n",
      "        Sequential: 3-7              147,713\n",
      "    Conv2d: 2-4                       66,048\n",
      "Sequential: 1-2                        --\n",
      "    Conv2d: 2-5                       65,664\n",
      "    ReLU: 2-6                         --\n",
      "    Dropout: 2-7                      --\n",
      "    Conv2d: 2-8                       258\n",
      "Sequential: 1-3                        --\n",
      "    Conv2d: 2-9                       65,664\n",
      "    ReLU: 2-10                        --\n",
      "    Dropout: 2-11                     --\n",
      "    Conv2d: 2-12                      16,512\n",
      "    ReLU: 2-13                        --\n",
      "    Dropout: 2-14                     --\n",
      "    Conv2d: 2-15                      258\n",
      "=================================================================\n",
      "Total params: 649,579\n",
      "Trainable params: 649,579\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_function_weather = FocalLoss(alpha=None, gamma=2.0, reduction='none', ignore_index=-100).to(device)\n",
    "loss_function_visibility = getattr(ordinal_losses, 'OrdinalEncoding')(K=N_CLASSES_VISIBILITY).to(device)\n",
    "n_outputs_visibility = loss_function_visibility.how_many_outputs()\n",
    "\n",
    "# network\n",
    "if NETWORK == \"MobileNetV3_ViT\":\n",
    "    net = ViT_FusionNet(num_classes_weather=N_CLASSES_WEATHER, num_classes_visibility=n_outputs_visibility, architecture=\"MobileNetV3_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=2, block_exp=2, n_layer=4).to(device)\n",
    "    \n",
    "    if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "        opt_weather = AdamW(list(net.encoders.parameters()) +\n",
    "                            list(net.transformer1.parameters()) +\n",
    "                            list(net.transformer2.parameters()) +\n",
    "                            list(net.change_channel_conv_camera.parameters()) +\n",
    "                            list(net.change_channel_conv_lidar.parameters()) +\n",
    "                            list(net.head_weather.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        opt_visibility = AdamW(list(net.encoders.parameters()) +\n",
    "                            list(net.transformer1.parameters()) +\n",
    "                            list(net.transformer2.parameters()) +\n",
    "                            list(net.change_channel_conv_camera.parameters()) +\n",
    "                            list(net.change_channel_conv_lidar.parameters()) +\n",
    "                            list(net.head_visibility.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        \n",
    "    elif OPT_TECHNIQUE == \"Weighted\":\n",
    "        opt = AdamW(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid optimisation technique.\"\n",
    "\n",
    "elif NETWORK == \"ResNet_ViT\":\n",
    "    net = ViT_FusionNet(num_classes_weather=N_CLASSES_WEATHER, num_classes_visibility=n_outputs_visibility, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2).to(device)\n",
    "    \n",
    "    if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "        opt_weather = AdamW(list(net.encoders.parameters()) +\n",
    "                            list(net.transformer1.parameters()) +\n",
    "                            list(net.transformer2.parameters()) +\n",
    "                            list(net.transformer3.parameters()) +\n",
    "                            list(net.transformer4.parameters()) +\n",
    "                            list(net.change_channel_conv_camera.parameters()) +\n",
    "                            list(net.change_channel_conv_lidar.parameters()) +\n",
    "                            list(net.head_weather.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        opt_visibility = AdamW(list(net.encoders.parameters()) +\n",
    "                            list(net.transformer1.parameters()) +\n",
    "                            list(net.transformer2.parameters()) +\n",
    "                            list(net.transformer3.parameters()) +\n",
    "                            list(net.transformer4.parameters()) +\n",
    "                            list(net.change_channel_conv_camera.parameters()) +\n",
    "                            list(net.change_channel_conv_lidar.parameters()) +\n",
    "                            list(net.head_visibility.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        \n",
    "    elif OPT_TECHNIQUE == \"Weighted\":\n",
    "        opt = AdamW(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid optimisation technique.\"\n",
    "\n",
    "elif NETWORK == \"MobileNetV3_Early\":\n",
    "    net = Mobile_Net_v3_Custom(3, N_CLASSES_WEATHER, n_outputs_visibility).to(device)\n",
    "\n",
    "    if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "        opt_weather = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_weather.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        opt_visibility = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_visibility.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "    \n",
    "    elif OPT_TECHNIQUE == \"Weighted\":\n",
    "        opt = AdamW(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid optimisation technique.\"\n",
    "\n",
    "elif NETWORK == \"MobileWeatherNet_Early\":\n",
    "    net = Mobile_Weather_Net_Custom(3, N_CLASSES_WEATHER, n_outputs_visibility).to(device)\n",
    "\n",
    "    if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "        opt_weather = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_weather.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        opt_visibility = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_visibility.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        \n",
    "    elif OPT_TECHNIQUE == \"Weighted\":\n",
    "        opt = AdamW(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid optimisation technique.\"\n",
    "\n",
    "elif NETWORK == \"RangeWeatherNet_Early\":\n",
    "    net = Range_Weather_Net_Custom(DarkResidualBlock, 3, N_CLASSES_WEATHER, n_outputs_visibility).to(device)\n",
    "\n",
    "    if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "        opt_weather = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_weather.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        opt_visibility = AdamW(list(net.backbone.parameters()) +\n",
    "                            list(net.head_visibility.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "        \n",
    "    elif OPT_TECHNIQUE == \"Weighted\":\n",
    "        opt = AdamW(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "    else:\n",
    "        assert False, \"Invalid optimisation technique.\"\n",
    "    \n",
    "else:\n",
    "    assert False, \"Invalid option. Valid options: MobileNetV3_ViT, ResNet_ViT, MobileNetV3_Early, RangeWeatherNet_Early, and MobileWeatherNet_Early.\"\n",
    "\n",
    "print(summary(net))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a52a0190-97b5-456f-a2db-1161bef2763e",
   "metadata": {},
   "source": [
    "## Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185238c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # training and validation cycles\n",
    "    print(\"[INFO] Network training and validation...\")\n",
    "    PATIENCE = int(0.8*EPOCHS)\n",
    "    vl_loss_min = 1e6\n",
    "    wait = 0\n",
    "    loss_avg_tr = []\n",
    "    loss_avg_vl = []\n",
    "\n",
    "    # loop over EPOCHS\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'* Epoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "        loss_total_tr = 0\n",
    "        loss_total_vl = 0\n",
    "\n",
    "        tic = time()\n",
    "        net.train()\n",
    "\n",
    "        for weather_gt_tr, visibility_gt_tr, camera_image_tr, lidar_image_range_tr, lidar_image_intensity_tr in tr: \n",
    "            weather_gt_tr = weather_gt_tr.to(device)\n",
    "            visibility_gt_tr = visibility_gt_tr.to(device)\n",
    "            camera_image_tr = camera_image_tr.to(device)\n",
    "            lidar_image_range_tr = lidar_image_range_tr.to(device)\n",
    "            lidar_image_intensity_tr = lidar_image_intensity_tr.to(device)\n",
    "\n",
    "            if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "                camera_image_tr = camera_image_tr[:,np.newaxis,:,:].float()\n",
    "                lidar_image_tr = torch.cat((lidar_image_range_tr[:,np.newaxis,:,:], lidar_image_intensity_tr[:,np.newaxis,:,:]), dim=1).float()\n",
    "                logits_weather_tr, logits_visibility_tr = net(camera_image_tr, lidar_image_tr)\n",
    "            else:\n",
    "                data_tr = torch.cat((camera_image_tr[:,np.newaxis,:,:], lidar_image_range_tr[:,np.newaxis,:,:], lidar_image_intensity_tr[:,np.newaxis,:,:]), dim=1).float()\n",
    "                logits_weather_tr, logits_visibility_tr = net(data_tr)\n",
    "            \n",
    "            proba_weather_tr = torch.nn.functional.softmax(logits_weather_tr, dim=1)\n",
    "\n",
    "            # forward\n",
    "            loss_weather_tr = loss_function_weather(proba_weather_tr, weather_gt_tr).mean()\n",
    "            loss_visibility_tr = loss_function_visibility(logits_visibility_tr, visibility_gt_tr).mean()\n",
    "            loss_total_tr += (loss_weather_tr.item() + loss_visibility_tr.item())\n",
    "\n",
    "            # backward\n",
    "            if OPT_TECHNIQUE == \"Multi_Adaptive\":\n",
    "                opt_weather.zero_grad()\n",
    "                loss_weather_tr.backward(retain_graph=True)\n",
    "                grads_weather_tr = [(param, param.grad.clone()) for param in net.parameters() if param.grad is not None]\n",
    "                \n",
    "                opt_visibility.zero_grad()\n",
    "                loss_visibility_tr.backward()\n",
    "                grads_visibility_tr = [(param, param.grad.clone()) for param in net.parameters() if param.grad is not None]\n",
    "\n",
    "                for param, grad in grads_weather_tr:\n",
    "                    param.grad = grad\n",
    "                opt_weather.step()\n",
    "\n",
    "                for param, grad in grads_visibility_tr:\n",
    "                    param.grad = grad\n",
    "                opt_visibility.step()\n",
    "\n",
    "            else:\n",
    "                loss_tr = (1.0 * loss_weather_tr) + (1.0 * loss_visibility_tr)\n",
    "                # zero the gradients\n",
    "                opt.zero_grad()\n",
    "                # compute gradients\n",
    "                loss_tr.backward()\n",
    "                # adjust learning weights\n",
    "                opt.step()\n",
    "            \n",
    "        toc = time()\n",
    "        print(f'  Elapsed training time: {toc-tic}s')\n",
    "\n",
    "        tic = time()\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for weather_gt_vl, visibility_gt_vl, camera_image_vl, lidar_image_range_vl, lidar_image_intensity_vl in vl:\n",
    "                weather_gt_vl = weather_gt_vl.to(device)\n",
    "                visibility_gt_vl = visibility_gt_vl.to(device)\n",
    "                camera_image_vl = camera_image_vl.to(device)\n",
    "                lidar_image_range_vl = lidar_image_range_vl.to(device)\n",
    "                lidar_image_intensity_vl = lidar_image_intensity_vl.to(device)\n",
    "\n",
    "                if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "                    camera_image_vl = camera_image_vl[:,np.newaxis,:,:].float()\n",
    "                    lidar_image_vl = torch.cat((lidar_image_range_vl[:,np.newaxis,:,:], lidar_image_intensity_vl[:,np.newaxis,:,:]), dim=1).float()\n",
    "                    logits_weather_vl, logits_visibility_vl = net(camera_image_vl, lidar_image_vl)\n",
    "                else:\n",
    "                    data_vl = torch.cat((camera_image_vl[:,np.newaxis,:,:], lidar_image_range_vl[:,np.newaxis,:,:], lidar_image_intensity_vl[:,np.newaxis,:,:]), dim=1).float()\n",
    "                    logits_weather_vl, logits_visibility_vl = net(data_vl)\n",
    "                \n",
    "                proba_weather_vl = torch.nn.functional.softmax(logits_weather_vl, dim=1)\n",
    "\n",
    "                # forward\n",
    "                loss_weather_vl = loss_function_weather(proba_weather_vl, weather_gt_vl).mean()\n",
    "                loss_visibility_vl = loss_function_visibility(logits_visibility_vl, visibility_gt_vl).mean()\n",
    "                loss_total_vl += (loss_weather_vl.item() + loss_visibility_vl.item())\n",
    "\n",
    "        toc = time()\n",
    "\n",
    "        loss_avg_tr.append(loss_total_tr / len(tr))\n",
    "        loss_avg_vl.append(loss_total_vl / len(vl))\n",
    "\n",
    "        print(f'  Elapsed validation time: {toc-tic}s')\n",
    "        print(f'  Tr Loss: {loss_avg_tr[epoch]}, Vl Loss: {loss_avg_vl[epoch]}')\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if loss_avg_vl[epoch] <= vl_loss_min:\n",
    "            print(f'  The best model was saved!')\n",
    "            torch.save(net, path_best_model)\n",
    "            vl_loss_min = loss_avg_vl[epoch]\n",
    "            wait = 0\n",
    "        # early stopping\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                print(f\"Terminated training for early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    print(f'The last model was saved!')\n",
    "    torch.save(net, path_last_model)\n",
    "\n",
    "    # plot loss\n",
    "    epochs_plot = range(1, (len(loss_avg_vl)+1))\n",
    "    plt.plot(epochs_plot, loss_avg_tr)\n",
    "    plt.plot(epochs_plot, loss_avg_vl)\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(epochs_plot)\n",
    "    plt.legend(('Training loss', 'Validation loss'), loc='upper right')\n",
    "    plt.savefig('MT_MM_W_MOR_Class_Train_Val_Loss_Network_{}_Optimization_{}_Seed_{}.pdf'.format(NETWORK, OPT_TECHNIQUE, SEED))\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16939702-a980-437f-9816-36b3a37fa337",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e331aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and analyze model memory footprint\n",
    "a = torch.cuda.memory_allocated(device)\n",
    "saved_model = torch.load(path_best_model, map_location=torch.device(device))\n",
    "b = torch.cuda.memory_allocated(device)\n",
    "\n",
    "print(\"Is the model on cuda: \", next(saved_model.parameters()).is_cuda)\n",
    "model_memory = (b - a)/(1024**2)\n",
    "print(\"Total memory of the model:\", model_memory, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the mean inference time\n",
    "camera_image_dummy = torch.randn(1, 1, 512, 960).to(device)\n",
    "lidar_image_dummy = torch.randn(1, 2, 512, 960).to(device)\n",
    "data_image_dummy = torch.cat((camera_image_dummy, lidar_image_dummy), dim=1)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 10000\n",
    "timings = np.zeros((repetitions, 1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # gpu-warm-up\n",
    "    for _ in range(100):\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            _ = saved_model(camera_image_dummy, lidar_image_dummy)\n",
    "        else:\n",
    "            _ = saved_model(data_image_dummy)\n",
    "    # measure performance\n",
    "    for rep in range(repetitions):\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            starter.record()\n",
    "            _ = saved_model(camera_image_dummy, lidar_image_dummy)\n",
    "            ender.record()\n",
    "        else:\n",
    "            starter.record()\n",
    "            _ = saved_model(data_image_dummy)\n",
    "            ender.record()\n",
    "        # wait for GPU sync\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Testing the network...\")\n",
    "saved_model.eval() # set model to evaluation mode\n",
    "\n",
    "metrics_weather = [torchmetrics.classification.MulticlassAccuracy(num_classes=N_CLASSES_WEATHER, average='weighted').to(device), torchmetrics.CohenKappa(task='multiclass', num_classes=N_CLASSES_WEATHER).to(device), torchmetrics.F1Score(task='multiclass', num_classes=N_CLASSES_WEATHER, average='weighted').to(device)]\n",
    "cm_weather = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=N_CLASSES_WEATHER, normalize='none').to(device)\n",
    "\n",
    "metrics_visibility = [torchmetrics.classification.MulticlassAccuracy(num_classes=N_CLASSES_VISIBILITY, average='weighted').to(device), torchmetrics.CohenKappa(task='multiclass', num_classes=N_CLASSES_VISIBILITY).to(device), torchmetrics.F1Score(task='multiclass', num_classes=N_CLASSES_VISIBILITY, average='weighted').to(device)]\n",
    "cm_visibility = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=N_CLASSES_VISIBILITY, normalize='none').to(device)\n",
    "\n",
    "tic = time()\n",
    "with torch.no_grad(): # turn off gradient tracking\n",
    "    for weather_gt_ts, visibility_gt_ts, camera_image_ts, lidar_image_range_ts, lidar_image_intensity_ts in ts:\n",
    "        weather_gt_ts = weather_gt_ts.to(device)\n",
    "        visibility_gt_ts = visibility_gt_ts.to(device)\n",
    "        camera_image_ts = camera_image_ts.to(device)\n",
    "        lidar_image_range_ts = lidar_image_range_ts.to(device)\n",
    "        lidar_image_intensity_ts = lidar_image_intensity_ts.to(device)\n",
    "\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            camera_image_ts = camera_image_ts[:,np.newaxis,:,:].float()\n",
    "            lidar_image_ts = torch.cat((lidar_image_range_ts[:,np.newaxis,:,:], lidar_image_intensity_ts[:,np.newaxis,:,:]), dim=1).float()\n",
    "            logits_weather_ts, logits_visibility_ts = saved_model(camera_image_ts, lidar_image_ts)    \n",
    "        else:\n",
    "            data_ts = torch.cat((camera_image_ts[:,np.newaxis,:,:], lidar_image_range_ts[:,np.newaxis,:,:], lidar_image_intensity_ts[:,np.newaxis,:,:]), dim=1).float()\n",
    "            logits_weather_ts, logits_visibility_ts = saved_model(data_ts)\n",
    "   \n",
    "        proba_weather_ts = torch.nn.functional.softmax(logits_weather_ts, dim=1)\n",
    "        class_weather_ts = torch.argmax(proba_weather_ts, dim=1)\n",
    "\n",
    "        class_visibility_ts = loss_function_visibility.to_classes(logits_visibility_ts)\n",
    "\n",
    "        for metric in metrics_weather:\n",
    "            metric.update(class_weather_ts, weather_gt_ts)\n",
    "        cm_weather.update(class_weather_ts, weather_gt_ts)\n",
    "\n",
    "        for metric in metrics_visibility:\n",
    "            metric.update(class_visibility_ts, visibility_gt_ts)\n",
    "        cm_visibility.update(class_visibility_ts, visibility_gt_ts)\n",
    "\n",
    "toc = time()\n",
    "print(f'Elapsed test time: {toc-tic}s')\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=2.0, w_pad=1.0, h_pad=5.0)\n",
    "sns.heatmap(cm_weather.compute().cpu(), ax=axs[0], annot=True, fmt='g', annot_kws={\"size\": 12})\n",
    "sns.heatmap(cm_visibility.compute().cpu(), ax=axs[1], annot=True, fmt='g', annot_kws={\"size\": 12})\n",
    "axs[0].set_title('Weather Classification', fontsize=18)\n",
    "axs[0].set_xlabel('Predicted Label', fontsize=16)\n",
    "axs[0].set_ylabel('True Label', fontsize=16)\n",
    "axs[0].xaxis.set_ticklabels(['Fog', 'Rain'], fontsize=12)\n",
    "axs[0].yaxis.set_ticklabels(['Fog', 'Rain'], fontsize=12, rotation=90)\n",
    "\n",
    "axs[1].set_title('MOR Classification', fontsize=18)\n",
    "axs[1].set_xlabel('Predicted Label', fontsize=16)\n",
    "axs[1].set_ylabel('True Label', fontsize=16)\n",
    "axs[1].xaxis.set_ticklabels(['0-40', '40-200', '>200'], fontsize=12)\n",
    "axs[1].yaxis.set_ticklabels(['0-40', '40-200', '>200'], fontsize=12, rotation=90)\n",
    "plt.savefig('MT_MM_W_MOR_Class_Confusion_Matrix_Network_{}_Optimization_{}_Seed_{}.pdf'.format(NETWORK, OPT_TECHNIQUE, SEED))\n",
    "plt.close()\n",
    "\n",
    "# save metrics\n",
    "with open('MT_MM_W_MOR_Class_Metrics_Network_{}_Optimization_{}_Seed_{}.csv'.format(NETWORK, OPT_TECHNIQUE, SEED),'w') as f:\n",
    "    writer = csv.writer(f, dialect='excel')\n",
    "    writer.writerow([\"Metric\", \"Value\"])\n",
    "    for metric in metrics_weather:\n",
    "        writer.writerow([\"Weather\" + str(metric.__class__.__name__), metric.compute().item()])\n",
    "\n",
    "    for metric in metrics_visibility:\n",
    "        writer.writerow([\"Visibility\" + str(metric.__class__.__name__), metric.compute().item()])\n",
    "\n",
    "    writer.writerow([\"ModelMemory\", model_memory])\n",
    "    writer.writerow([\"MeanInferenceTime\", mean_syn])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

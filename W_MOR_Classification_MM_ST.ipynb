{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b2e131-b3de-4150-85ba-62cf69b2ebe2",
   "metadata": {},
   "source": [
    "# Weather and MOR Classification for Autonomous Driving - Single-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "995e4d06-ccd5-4375-b858-6e500a3419de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import io, util\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.color import rgb2hsv, rgb2gray, rgb2yuv\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, io, models\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torchinfo import summary\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import json\n",
    "import seaborn as sns\n",
    "from pyquaternion import Quaternion\n",
    "import fnmatch\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import ordinal_losses_theia as ordinal_losses # ordinal losses from https://github.com/rpmcruz/ordinal-losses\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b963a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging purposes\n",
    "# class Args:\n",
    "#     BATCH=8\n",
    "#     EPOCHS=1\n",
    "#     NETWORK=\"MobileNetV3_Early\" # \"MobileNetV3_ViT\", \"ResNet_ViT\", and \"MobileNetV3_Early\"\n",
    "#     TRAIN_MODEL=True\n",
    "#     SEED=1998\n",
    "# args=Args()\n",
    "\n",
    "def str_to_bool(value):\n",
    "    if value.lower() in ('yes', 'true', '1'):\n",
    "        return True\n",
    "    elif value.lower() in ('no', 'false', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Invalid value for boolean argument. Accepted values are \"yes\"/\"true\"/\"1\" or \"no\"/\"false\"/\"0\".')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('BATCH', type=int)\n",
    "parser.add_argument('EPOCHS', type=int)\n",
    "parser.add_argument('NETWORK', type=str)\n",
    "parser.add_argument('TRAIN_MODEL', type=str_to_bool, help='Specify whether to train the model (yes/true/1) or not (no/false/0)')\n",
    "parser.add_argument('SEED', type=int)\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c83600",
   "metadata": {},
   "source": [
    "## System Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30e8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system variables\n",
    "BATCH_SIZE = args.BATCH\n",
    "EPOCHS = args.EPOCHS\n",
    "TRAIN_MODEL = args.TRAIN_MODEL\n",
    "NETWORK = args.NETWORK\n",
    "SEED = args.SEED\n",
    "NUM_TOTAL_SAMPLES = 1293\n",
    "IMAGE_HEIGHT = 1024\n",
    "IMAGE_WIDTH = 1920\n",
    "MEAN = np.array([0.5, 0.5, 0.5])\n",
    "STD = np.array([0.5, 0.5, 0.5])\n",
    "N_CLASSES_WEATHER = 2\n",
    "N_CLASSES_VISIBILITY = 3\n",
    "VISIBILITY_BINS = np.array([40.0, 200.0])\n",
    "# path for saving the models\n",
    "path_best_weather_model = \"./ST_MM_W_MOR_Class_Best_Weather_Network_{}_Seed_{}.pth\".format(NETWORK, SEED)\n",
    "path_best_visibility_model = \"./ST_MM_W_MOR_Class_Best_Visibility_Network_{}_Seed_{}.pth\".format(NETWORK, SEED)\n",
    "path_last_weather_model = \"./ST_MM_W_MOR_Class_Last_Weather_Network_{}_Seed_{}.pth\".format(NETWORK, SEED)\n",
    "path_last_visibility_model = \"./ST_MM_W_MOR_Class_Last_Visibility_Network_{}_Seed_{}.pth\".format(NETWORK, SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d3eee2-6aea-499f-83e1-b82526b406d1",
   "metadata": {},
   "source": [
    "## Transformations / Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a0a4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_aug = A.Compose([A.CenterCrop(int(IMAGE_HEIGHT*0.5), int(IMAGE_WIDTH*0.5), p=1.0),\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.Affine(scale=(1.1,1.25), keep_ratio=True, p=0.5),\n",
    "                        A.Normalize(mean=MEAN, std=STD, always_apply=True),\n",
    "                        ToTensorV2()],\n",
    "                        additional_targets={'mask2': 'mask', 'mask3': 'mask'})\n",
    "\n",
    "transform_base = A.Compose([A.CenterCrop(int(IMAGE_HEIGHT*0.5), int(IMAGE_WIDTH*0.5), p=1.0),\n",
    "                            A.Normalize(mean=MEAN, std=STD, always_apply=True),\n",
    "                            ToTensorV2()],\n",
    "                            additional_targets={'mask2': 'mask', 'mask3': 'mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55d19d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FogChamber(Dataset):\n",
    "    def load_calib_data(self, path_total_dataset, name_camera_calib, tf_tree, velodyne_name='lidar_hdl64_s3_roof'):\n",
    "        assert velodyne_name in ['lidar_hdl64_s3_roof', 'lidar_vlp32_roof'], 'wrong frame id in tf_tree for velodyne_name'\n",
    "\n",
    "        with open(os.path.join(path_total_dataset, name_camera_calib), 'r') as f:\n",
    "            data_camera = json.load(f)\n",
    "        with open(os.path.join(path_total_dataset, tf_tree), 'r') as f:\n",
    "            data_extrinsics = json.load(f)\n",
    "\n",
    "        calib_dict = {'calib_cam_stereo_left.json': 'cam_stereo_left_optical',\n",
    "                    'calib_cam_stereo_right.json': 'cam_stereo_right_optical',\n",
    "                    'calib_gated_bwv.json': 'bwv_cam_optical'}\n",
    "\n",
    "        cam_name = calib_dict[name_camera_calib]\n",
    "\n",
    "        # scan data extrinsics for transformation from lidar to camera\n",
    "        important_translations = [velodyne_name, 'radar', cam_name]\n",
    "        translations = []\n",
    "\n",
    "        for item in data_extrinsics:\n",
    "            if item['child_frame_id'] in important_translations:\n",
    "                translations.append(item)\n",
    "                if item['child_frame_id'] == cam_name:\n",
    "                    T_cam = item['transform']\n",
    "                elif item['child_frame_id'] == velodyne_name:\n",
    "                    T_velodyne = item['transform']\n",
    "                elif item['child_frame_id'] == 'radar':\n",
    "                    T_radar = item['transform']\n",
    "\n",
    "        # use pyquaternion to setup rotation matrices properly\n",
    "        R_c_quaternion = Quaternion(w=T_cam['rotation']['w'] * 360 / 2 / np.pi, x=T_cam['rotation']['x'] * 360 / 2 / np.pi,\n",
    "                                    y=T_cam['rotation']['y'] * 360 / 2 / np.pi, z=T_cam['rotation']['z'] * 360 / 2 / np.pi)\n",
    "        R_v_quaternion = Quaternion(w=T_velodyne['rotation']['w'] * 360 / 2 / np.pi,\n",
    "                                    x=T_velodyne['rotation']['x'] * 360 / 2 / np.pi,\n",
    "                                    y=T_velodyne['rotation']['y'] * 360 / 2 / np.pi,\n",
    "                                    z=T_velodyne['rotation']['z'] * 360 / 2 / np.pi)\n",
    "\n",
    "        # setup quaternion values as 3x3 orthogonal rotation matrices\n",
    "        R_c_matrix = R_c_quaternion.rotation_matrix\n",
    "        R_v_matrix = R_v_quaternion.rotation_matrix\n",
    "\n",
    "        # setup translation Vectors\n",
    "        Tr_cam = np.asarray([T_cam['translation']['x'], T_cam['translation']['y'], T_cam['translation']['z']])\n",
    "        Tr_velodyne = np.asarray([T_velodyne['translation']['x'], T_velodyne['translation']['y'], T_velodyne['translation']['z']])\n",
    "\n",
    "        # setup Translation Matrix camera to lidar -> ROS spans transformation from its children to its parents therefore one inversion step is needed for zero_to_camera -> <parent_child>\n",
    "        zero_to_camera = np.zeros((3, 4))\n",
    "        zero_to_camera[0:3, 0:3] = R_c_matrix\n",
    "        zero_to_camera[0:3, 3] = Tr_cam\n",
    "        zero_to_camera = np.vstack((zero_to_camera, np.array([0, 0, 0, 1])))\n",
    "\n",
    "        zero_to_velodyne = np.zeros((3, 4))\n",
    "        zero_to_velodyne[0:3, 0:3] = R_v_matrix\n",
    "        zero_to_velodyne[0:3, 3] = Tr_velodyne\n",
    "        zero_to_velodyne = np.vstack((zero_to_velodyne, np.array([0, 0, 0, 1])))\n",
    "\n",
    "        # calculate total extrinsic transformation to camera\n",
    "        velodyne_to_camera = np.matmul(np.linalg.inv(zero_to_camera), zero_to_velodyne)\n",
    "        camera_to_velodyne = np.matmul(np.linalg.inv(zero_to_velodyne), zero_to_camera)\n",
    "\n",
    "        # read projection matrix P and camera rectification matrix R\n",
    "        P = np.reshape(data_camera['P'], [3, 4])\n",
    "\n",
    "        # rectification matrix R has to be equal to the identity as the projection matrix P contains the R matrix w.r.t KITTI definition\n",
    "        R = np.identity(4)\n",
    "\n",
    "        # calculate total transformation matrix from velodyne to camera\n",
    "        vtc = np.matmul(np.matmul(P, R), velodyne_to_camera)\n",
    "        return velodyne_to_camera, camera_to_velodyne, P, R, vtc, zero_to_camera\n",
    "\n",
    "    def py_func_project_3D_to_2D(self, points_3D, P):\n",
    "        # project on image\n",
    "        points_2D = np.matmul(P, np.vstack((points_3D, np.ones([1, np.shape(points_3D)[1]]))))\n",
    "\n",
    "        # scale projected points\n",
    "        points_2D[0][:] = points_2D[0][:] / points_2D[2][:]\n",
    "        points_2D[1][:] = points_2D[1][:] / points_2D[2][:]\n",
    "\n",
    "        points_2D = points_2D[0:2]\n",
    "        return points_2D.transpose()\n",
    "\n",
    "    def weather_digitize(self, weather_data):\n",
    "        weather_in_bins = np.array([])\n",
    "        if weather_data == 'Fog Small Droplets':\n",
    "            weather_in_bins = 0\n",
    "        elif weather_data == 'Rain':\n",
    "            weather_in_bins = 1\n",
    "        else:\n",
    "            assert False, \"Invalid weather class.\"\n",
    "        return weather_in_bins\n",
    "\n",
    "    def files_with_equal_names_in_diff_dir(self, dir1, dir2, dir3, ext1, ext2, ext3):\n",
    "        files1 = os.listdir(dir1)\n",
    "        files2 = os.listdir(dir2)\n",
    "        files3 = os.listdir(dir3)\n",
    "        matches = []\n",
    "\n",
    "        for file1 in files1:\n",
    "            if fnmatch.fnmatch(file1, '*' + ext1):\n",
    "                name1 = os.path.splitext(file1)[0]\n",
    "                for file2 in files2:\n",
    "                    if fnmatch.fnmatch(file2, name1 + ext2):\n",
    "                        for file3 in files3:\n",
    "                            if fnmatch.fnmatch(file3, name1 + ext3):\n",
    "                                matches.append((os.path.join(dir1, file1), os.path.join(dir2, file2), os.path.join(dir3, file3)))\n",
    "        return matches\n",
    "    \n",
    "    def __init__(self, root, subset, rgb_fold, lidar_fold, weather_visibility_fold, dict_transform=None):\n",
    "        _, _, _, _, self.vtc,_ = self.load_calib_data(root, 'calib_cam_stereo_left.json', 'calib_tf_tree_full.json', 'lidar_hdl64_s3_roof')\n",
    "        self.subset = subset\n",
    "\n",
    "        assert rgb_fold in ('cam_stereo_left_lut')\n",
    "        self.rgb_path = os.path.join(root, rgb_fold)\n",
    "        self.rgb_extension = '.png'\n",
    "\n",
    "        assert lidar_fold in ('lidar_hdl64_strongest', 'lidar_hdl64_last')\n",
    "        self.lidar_path = os.path.join(root, lidar_fold)\n",
    "        self.lidar_extension = '.bin'\n",
    "\n",
    "        assert weather_visibility_fold in ('cerema')\n",
    "        self.weather_visibility_path = os.path.join(root, weather_visibility_fold)\n",
    "        self.weather_visibility_extension = '.json'\n",
    "\n",
    "        self.file_matches = sorted(self.files_with_equal_names_in_diff_dir(self.rgb_path, self.lidar_path, self.weather_visibility_path, self.rgb_extension, self.lidar_extension, self.weather_visibility_extension))\n",
    "        self.rgb_files = [i[0] for i in self.file_matches]\n",
    "        self.lidar_files = [i[1] for i in self.file_matches]\n",
    "        self.weather_visibility_files = [i[2] for i in self.file_matches]\n",
    "\n",
    "        self.dict_transform = dict_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i, only_weather=False, only_visibility=False):\n",
    "        j = self.subset[i]\n",
    "\n",
    "        if only_weather:\n",
    "            # get weather GT\n",
    "            weather_visibility_fname = self.weather_visibility_files[j]\n",
    "            weather_data = json.load(open(weather_visibility_fname))['weather']\n",
    "            weather_in_bins = self.weather_digitize(weather_data)\n",
    "            return weather_in_bins\n",
    "        \n",
    "        if only_visibility:\n",
    "            # get visibility GT\n",
    "            weather_visibility_fname = self.weather_visibility_files[j]\n",
    "            visibility_data = json.load(open(weather_visibility_fname))['metereological_visibility']\n",
    "            visibility_in_bins = np.digitize(visibility_data, VISIBILITY_BINS)\n",
    "            return visibility_in_bins\n",
    "    \n",
    "        # get LUT RGB image\n",
    "        rgb_fname = self.rgb_files[j]\n",
    "        rgb_image = imread(rgb_fname)\n",
    "\n",
    "        # image entropy (normalised)\n",
    "        gray = rgb2gray(rgb_image)\n",
    "        image_entropy = entropy(util.img_as_ubyte(gray), disk(5)) / 6.1917 # 6.1917 is the max image entropy for disk 5\n",
    "\n",
    "        # load and parse lidar data\n",
    "        lidar_fname = self.lidar_files[j]\n",
    "        lidar_data_raw = np.fromfile(lidar_fname, dtype=np.float32).reshape((-1, 5))\n",
    "        # filter away all points behind image plane and below distance threshold\n",
    "        r = np.sqrt(lidar_data_raw[:, 0] ** 2 + lidar_data_raw[:, 1] ** 2 + lidar_data_raw[:, 2] ** 2)\n",
    "        lidar_data_raw = lidar_data_raw[np.where(r > 1.5)]\n",
    "        lidar_data_raw = lidar_data_raw[np.where(lidar_data_raw[:, 0] > 2.5)]\n",
    "        # range calculation\n",
    "        lidar_range = np.sqrt(lidar_data_raw[:, 0] ** 2 + lidar_data_raw[:, 1] ** 2 + lidar_data_raw[:, 2] ** 2)\n",
    "        # 3D to 2D valid coordinates\n",
    "        points_2D = self.py_func_project_3D_to_2D(lidar_data_raw[:, 0:3].transpose(), self.vtc)\n",
    "        within_image_boarder_width = np.logical_and(IMAGE_WIDTH > points_2D[:, 0], points_2D[:, 0] >= 0)                   \n",
    "        within_image_boarder_height = np.logical_and(IMAGE_HEIGHT > points_2D[:, 1], points_2D[:, 1] >= 0)\n",
    "        valid_points = np.logical_and(within_image_boarder_width, within_image_boarder_height)\n",
    "        coordinates = np.where(valid_points)[0]\n",
    "        img_coordinates = points_2D[coordinates, :].astype(dtype=np.int32)\n",
    "        # lidar range image (normalised)\n",
    "        image_lidar_range = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image_lidar_range[img_coordinates[:, 0], img_coordinates[:, 1]] = lidar_range[coordinates] / 109.2780 # 109.2780 is the max range\n",
    "        image_lidar_range = image_lidar_range.transpose()\n",
    "        # lidar intensity image (normalised)\n",
    "        image_lidar_intensity = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image_lidar_intensity[img_coordinates[:, 0], img_coordinates[:, 1]] = lidar_data_raw[:,3][coordinates] / 255 # 255 is the max intensity\n",
    "        image_lidar_intensity = image_lidar_intensity.transpose()\n",
    "\n",
    "        # get weather and visibility GT\n",
    "        weather_visibility_fname = self.weather_visibility_files[j]\n",
    "        weather_data = json.load(open(weather_visibility_fname))['weather']\n",
    "        weather_in_bins = self.weather_digitize(weather_data)\n",
    "        visibility_data = json.load(open(weather_visibility_fname))['metereological_visibility']\n",
    "        visibility_in_bins = np.digitize(visibility_data, VISIBILITY_BINS)\n",
    "\n",
    "        if self.dict_transform:\n",
    "            transformed = self.dict_transform(image=rgb_image, mask=image_lidar_range, mask2=image_lidar_intensity, mask3=image_entropy)\n",
    "            image_lidar_range = transformed['mask']\n",
    "            image_lidar_intensity = transformed['mask2']\n",
    "            image_entropy = transformed['mask3']\n",
    "        \n",
    "        return weather_in_bins, visibility_in_bins, image_entropy, image_lidar_range, image_lidar_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d22aa7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[704  71]\n"
     ]
    }
   ],
   "source": [
    "# to ensure reproducibility\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "\n",
    "num_tr_samples = int(0.6 * NUM_TOTAL_SAMPLES)\n",
    "num_vl_samples = int(0.2 * NUM_TOTAL_SAMPLES)\n",
    "num_ts_samples = int(0.2 * NUM_TOTAL_SAMPLES)\n",
    "data = list(range(0, NUM_TOTAL_SAMPLES))\n",
    "random.shuffle(data) # does NOT prevent leakage between sequences, but NO better solution has been found\n",
    "\n",
    "tr_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[:num_tr_samples], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_aug)\n",
    "vl_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[num_tr_samples:num_tr_samples + num_vl_samples], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_base)\n",
    "ts_dataset = FogChamber(r'/data/auto/DENSE/FogchamberDataset/', data[num_tr_samples + num_vl_samples:], 'cam_stereo_left_lut', 'lidar_hdl64_strongest', 'cerema', transform_base)\n",
    "\n",
    "# manage class imbalance\n",
    "weather_samples = [tr_dataset.__getitem__(i, only_weather=True) for i in range(len(tr_dataset))]\n",
    "_, weather_class_sample_count = np.unique(weather_samples, return_counts=True)\n",
    "print(weather_class_sample_count)\n",
    "weather_class_weights = 1 / torch.tensor(weather_class_sample_count)\n",
    "weight_per_sample = weather_class_weights[weather_samples]\n",
    "weather_sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_per_sample, len(weight_per_sample))\n",
    "\n",
    "tr = DataLoader(tr_dataset, BATCH_SIZE, sampler=weather_sampler, num_workers=16, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "vl = DataLoader(vl_dataset, BATCH_SIZE, num_workers=16, shuffle=False, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "ts = DataLoader(ts_dataset, BATCH_SIZE, num_workers=16, shuffle=False, pin_memory=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7193333e-5b64-42d0-ac1a-79956966d62f",
   "metadata": {},
   "source": [
    "## Network and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV3 Custom - Single-Task\n",
    "class Mobile_Net_v3_Custom_Weather(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes_weather, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        self.backbone_weather = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_weather.features[0][0] = nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.backbone_weather.avgpool = nn.Sequential(nn.Conv2d(self.backbone_weather.features[12][0].out_channels, perception_output_features, (1, 1)),\n",
    "                                                    nn.AdaptiveAvgPool2d(output_size=1))\n",
    "        self.backbone_weather.classifier = nn.Identity()\n",
    "\n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_weather = self.backbone_weather(x)\n",
    "        # reshape the backbone output to (batch_size, n_features, 1, 1)\n",
    "        out_weather = out_weather.view(out_weather.size(0), -1, 1, 1)\n",
    "        # forward pass through the weather head\n",
    "        out_weather = self.head_weather(out_weather).view(out_weather.size(0), -1)\n",
    "        return out_weather\n",
    "    \n",
    "class Mobile_Net_v3_Custom_Visibility(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes_visibility, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        self.backbone_visibility = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_visibility.features[0][0] = nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.backbone_visibility.avgpool = nn.Sequential(nn.Conv2d(self.backbone_visibility.features[12][0].out_channels, perception_output_features, (1, 1)),\n",
    "                                                        nn.AdaptiveAvgPool2d(output_size=1))\n",
    "        self.backbone_visibility.classifier = nn.Identity()\n",
    "\n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_visibility = self.backbone_visibility(x)\n",
    "        out_visibility = out_visibility.view(out_visibility.size(0), -1, 1, 1)\n",
    "        out_visibility = self.head_visibility(out_visibility).view(out_visibility.size(0), -1)\n",
    "        return out_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28fdebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head masked self-attention layer with a projection at the end\n",
    "# code based on the article \"Multi-Modal Fusion Transformer for End-to-End Autonomous Driving\", Aditya Prakash, Kashyap Chitta, Andreas Geiger, 2021\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerBlock(nn.Module): # transformer block\n",
    "    def __init__(self, n_embd, n_head, block_exp, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MSA(n_embd, n_head, attn_pdrop, resid_pdrop)\n",
    "        self.mlp = nn.Sequential(nn.Linear(n_embd, block_exp * n_embd),\n",
    "                                nn.ReLU(True), # changed from GELU\n",
    "                                nn.Linear(block_exp * n_embd, n_embd),\n",
    "                                nn.Dropout(resid_pdrop))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_exp, n_layer, camera_vert_anchors, camera_horz_anchors, lidar_vert_anchors, lidar_horz_anchors, seq_len, embd_pdrop, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len # only support seq len 1\n",
    "        self.camera_vert_anchors = camera_vert_anchors\n",
    "        self.camera_horz_anchors = camera_horz_anchors\n",
    "        self.lidar_vert_anchors = lidar_vert_anchors\n",
    "        self.lidar_horz_anchors = lidar_horz_anchors\n",
    "        # positional embedding parameter (learnable), camera + lidar\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.seq_len * camera_vert_anchors * camera_horz_anchors + self.seq_len * lidar_vert_anchors * lidar_horz_anchors, n_embd))\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, block_exp, attn_pdrop, resid_pdrop) for layer in range(n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.block_size = self.seq_len\n",
    "\n",
    "    def forward(self, camera_tensor, lidar_tensor):\n",
    "        bz = lidar_tensor.shape[0]\n",
    "        lidar_h, lidar_w = lidar_tensor.shape[2:4]\n",
    "        camera_h, camera_w = camera_tensor.shape[2:4]\n",
    "\n",
    "        assert self.seq_len == 1\n",
    "        camera_tensor = camera_tensor.view(bz, self.seq_len, -1, camera_h, camera_w).permute(0,1,3,4,2).contiguous().view(bz, -1, self.n_embd)\n",
    "        lidar_tensor = lidar_tensor.view(bz, self.seq_len, -1, lidar_h, lidar_w).permute(0,1,3,4,2).contiguous().view(bz, -1, self.n_embd)\n",
    "        token_embeddings = torch.cat((camera_tensor, lidar_tensor), dim=1)\n",
    "\n",
    "        x = self.drop(self.pos_emb + token_embeddings)\n",
    "        x = self.blocks(x) # (B, an * T, C)\n",
    "        x = self.ln_f(x) # (B, an * T, C)\n",
    "        x = x.view(bz, self.seq_len * self.camera_vert_anchors * self.camera_horz_anchors + self.seq_len * self.lidar_vert_anchors * self.lidar_horz_anchors, self.n_embd)\n",
    "\n",
    "        camera_tensor_out = x[:, :self.seq_len*self.camera_vert_anchors*self.camera_horz_anchors, :].contiguous().view(bz * self.seq_len, -1, camera_h, camera_w)\n",
    "        lidar_tensor_out = x[:, self.seq_len*self.camera_vert_anchors*self.camera_horz_anchors:, :].contiguous().view(bz * self.seq_len, -1, lidar_h, lidar_w)\n",
    "\n",
    "        return camera_tensor_out, lidar_tensor_out\n",
    "    \n",
    "class MobileNetV3_Encoder_Weather(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera_weather = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_camera_weather.features[0][0] = nn.Conv2d(camera_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.camera_n_features_weather = self.backbone_camera_weather.classifier[0].in_features\n",
    "        self.backbone_camera_weather.classifier = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar_weather = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_lidar_weather.features[0][0] = nn.Conv2d(lidar_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.lidar_n_features_weather = self.backbone_lidar_weather.classifier[0].in_features\n",
    "        self.backbone_lidar_weather.classifier = nn.Identity()\n",
    "\n",
    "class MobileNetV3_Encoder_Visibility(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera_visibility = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_camera_visibility.features[0][0] = nn.Conv2d(camera_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.camera_n_features_visibility = self.backbone_camera_visibility.classifier[0].in_features\n",
    "        self.backbone_camera_visibility.classifier = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar_visibility = torchvision.models.mobilenet_v3_small(weights=None)\n",
    "        self.backbone_lidar_visibility.features[0][0] = nn.Conv2d(lidar_in_shape, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.lidar_n_features_visibility = self.backbone_lidar_visibility.classifier[0].in_features\n",
    "        self.backbone_lidar_visibility.classifier = nn.Identity()\n",
    "    \n",
    "class ResNet_Encoder_Weather(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera_weather = torchvision.models.resnet34(weights=None)\n",
    "        self.backbone_camera_weather.conv1 = nn.Conv2d(camera_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.camera_n_features_weather = self.backbone_camera_weather.fc.in_features\n",
    "        self.backbone_camera_weather.fc = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar_weather = torchvision.models.resnet18(weights=None)\n",
    "        self.backbone_lidar_weather.conv1 = nn.Conv2d(lidar_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.lidar_n_features_weather = self.backbone_lidar_weather.fc.in_features\n",
    "        self.backbone_lidar_weather.fc = nn.Identity()\n",
    "\n",
    "class ResNet_Encoder_Visibility(nn.Module):\n",
    "    def __init__(self, camera_in_shape, lidar_in_shape):\n",
    "        super().__init__()\n",
    "        self.backbone_camera_visibility = torchvision.models.resnet34(weights=None)\n",
    "        self.backbone_camera_visibility.conv1 = nn.Conv2d(camera_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.camera_n_features_visibility = self.backbone_camera_visibility.fc.in_features\n",
    "        self.backbone_camera_visibility.fc = nn.Identity()\n",
    "\n",
    "        self.backbone_lidar_visibility = torchvision.models.resnet18(weights=None)\n",
    "        self.backbone_lidar_visibility.conv1 = nn.Conv2d(lidar_in_shape, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.lidar_n_features_visibility = self.backbone_lidar_visibility.fc.in_features\n",
    "        self.backbone_lidar_visibility.fc = nn.Identity()\n",
    "\n",
    "class ViT_FusionNet_Weather(nn.Module):\n",
    "    def __init__(self, num_classes_weather, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=4, block_exp=4, n_layer=8, seq_len=1, embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.architecture = architecture\n",
    "\n",
    "        camera_vert_anchors = 16\n",
    "        camera_horz_anchors = 30\n",
    "        lidar_vert_anchors = 16\n",
    "        lidar_horz_anchors = 30\n",
    "\n",
    "        self.avgpool_camera_weather = nn.AdaptiveAvgPool2d((camera_vert_anchors, camera_horz_anchors))\n",
    "        self.avgpool_lidar_weather = nn.AdaptiveAvgPool2d((lidar_vert_anchors, lidar_horz_anchors))\n",
    "\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            self.encoders = ResNet_Encoder_Weather(camera_in_channels, lidar_in_channels)\n",
    "\n",
    "            # n_embd (int) - number of expected features in the encoder/decoder inputs, n_head (int) - number of heads in the multiheadattention models\n",
    "            self.transformer1_weather = GPT(n_embd=self.encoders.backbone_camera_weather.layer1[1].conv2.out_channels, # note: in_channels can also be accessed\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2_weather = GPT(n_embd=self.encoders.backbone_camera_weather.layer2[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer3_weather = GPT(n_embd=self.encoders.backbone_camera_weather.layer3[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer4_weather = GPT(n_embd=self.encoders.backbone_camera_weather.layer4[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "            \n",
    "            if self.encoders.backbone_camera_weather.layer4[1].conv2.out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera_weather = nn.Conv2d(self.encoders.backbone_camera_weather.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar_weather = nn.Conv2d(self.encoders.backbone_camera_weather.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera_weather = nn.Sequential()\n",
    "                self.change_channel_conv_lidar_weather = nn.Sequential()\n",
    "\n",
    "        elif self.architecture == \"MobileNetV3_ViT\":\n",
    "            self.encoders = MobileNetV3_Encoder_Weather(camera_in_channels, lidar_in_channels)\n",
    "\n",
    "            self.transformer1_weather = GPT(n_embd=self.encoders.backbone_camera_weather.features[6].block[3][0].out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2_weather = GPT(n_embd=self.encoders.backbone_camera_weather.features[12][0].out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            if self.encoders.backbone_camera_weather.features[12][0].out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera_weather = nn.Conv2d(self.encoders.backbone_camera_weather.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar_weather = nn.Conv2d(self.encoders.backbone_camera_weather.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera_weather = nn.Sequential()\n",
    "                self.change_channel_conv_lidar_weather = nn.Sequential()\n",
    "                \n",
    "        else:\n",
    "            assert False, \"Invalid architecture.\"\n",
    "\n",
    "        # classification head\n",
    "        self.head_weather = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Conv2d(128, num_classes_weather, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, in_camera, in_lidar):\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.conv1(in_camera)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.bn1(camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.relu(camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.maxpool(camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.conv1(in_lidar)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.bn1(lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.relu(lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.maxpool(lidar_features_weather)\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.layer1(camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.layer1(lidar_features_weather)\n",
    "            camera_features_layer1_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_layer1_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_layer1_weather, lidar_features_layer1_weather = self.transformer1_weather(camera_features_layer1_weather, lidar_features_layer1_weather)\n",
    "            camera_features_layer1_weather = F.interpolate(camera_features_layer1_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer1_weather = F.interpolate(lidar_features_layer1_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_layer1_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_layer1_weather\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.layer2(camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.layer2(lidar_features_weather)\n",
    "            camera_features_layer2_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_layer2_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_layer2_weather, lidar_features_layer2_weather = self.transformer2_weather(camera_features_layer2_weather, lidar_features_layer2_weather)\n",
    "            camera_features_layer2_weather = F.interpolate(camera_features_layer2_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer2_weather = F.interpolate(lidar_features_layer2_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_layer2_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_layer2_weather\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.layer3(camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.layer3(lidar_features_weather)\n",
    "            camera_features_layer3_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_layer3_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_layer3_weather, lidar_features_layer3_weather = self.transformer3_weather(camera_features_layer3_weather, lidar_features_layer3_weather)\n",
    "            camera_features_layer3_weather = F.interpolate(camera_features_layer3_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer3_weather = F.interpolate(lidar_features_layer3_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_layer3_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_layer3_weather\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.layer4(camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.layer4(lidar_features_weather)\n",
    "            camera_features_layer4_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_layer4_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_layer4_weather, lidar_features_layer4_weather = self.transformer4_weather(camera_features_layer4_weather, lidar_features_layer4_weather)\n",
    "            camera_features_layer4_weather = F.interpolate(camera_features_layer4_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer4_weather = F.interpolate(lidar_features_layer4_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_layer4_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_layer4_weather\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features_weather = self.change_channel_conv_camera_weather(camera_features_weather)\n",
    "            lidar_features_weather = self.change_channel_conv_lidar_weather(lidar_features_weather)\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.avgpool(camera_features_weather)\n",
    "            camera_features_weather = torch.flatten(camera_features_weather, 1)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.avgpool(lidar_features_weather)\n",
    "            lidar_features_weather = torch.flatten(lidar_features_weather, 1)\n",
    "            fused_features_weather = camera_features_weather + lidar_features_weather\n",
    "        \n",
    "        else:\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[0](in_camera)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[1](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[2](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[3](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[4](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[5](camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[0](in_lidar)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[1](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[2](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[3](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[4](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[5](lidar_features_weather)\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[6](camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[6](lidar_features_weather)\n",
    "            camera_features_block1_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_block1_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_block1_weather, lidar_features_block1_weather = self.transformer1_weather(camera_features_block1_weather, lidar_features_block1_weather)\n",
    "            camera_features_block1_weather = F.interpolate(camera_features_block1_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block1_weather = F.interpolate(lidar_features_block1_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_block1_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_block1_weather\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[7](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[8](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[9](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[10](camera_features_weather)\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[11](camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[7](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[8](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[9](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[10](lidar_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[11](lidar_features_weather)\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.features[12](camera_features_weather)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.features[12](lidar_features_weather)\n",
    "            camera_features_block2_weather = self.avgpool_camera_weather(camera_features_weather)\n",
    "            lidar_features_block2_weather = self.avgpool_lidar_weather(lidar_features_weather)\n",
    "            camera_features_block2_weather, lidar_features_block2_weather = self.transformer2_weather(camera_features_block2_weather, lidar_features_block2_weather)\n",
    "            camera_features_block2_weather = F.interpolate(camera_features_block2_weather, size=(camera_features_weather.shape[2], camera_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block2_weather = F.interpolate(lidar_features_block2_weather, size=(lidar_features_weather.shape[2], lidar_features_weather.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_weather = camera_features_weather + camera_features_block2_weather\n",
    "            lidar_features_weather = lidar_features_weather + lidar_features_block2_weather\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features_weather = self.change_channel_conv_camera_weather(camera_features_weather)\n",
    "            lidar_features_weather = self.change_channel_conv_lidar_weather(lidar_features_weather)\n",
    "\n",
    "            camera_features_weather = self.encoders.backbone_camera_weather.avgpool(camera_features_weather)\n",
    "            camera_features_weather = torch.flatten(camera_features_weather, 1)\n",
    "            lidar_features_weather = self.encoders.backbone_lidar_weather.avgpool(lidar_features_weather)\n",
    "            lidar_features_weather = torch.flatten(lidar_features_weather, 1)\n",
    "            fused_features_weather = camera_features_weather + lidar_features_weather\n",
    "\n",
    "        # reshape the output to (batch_size, n_features, 1, 1)\n",
    "        fused_features_weather = fused_features_weather.view(fused_features_weather.size(0), -1, 1, 1)\n",
    "        # forward pass through the weather head\n",
    "        out_weather = self.head_weather(fused_features_weather).view(fused_features_weather.size(0), -1)\n",
    "        return out_weather\n",
    "\n",
    "class ViT_FusionNet_Visibility(nn.Module):\n",
    "    def __init__(self, num_classes_visibility, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=4, block_exp=4, n_layer=8, seq_len=1, embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1, perception_output_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.architecture = architecture\n",
    "\n",
    "        camera_vert_anchors = 16\n",
    "        camera_horz_anchors = 30\n",
    "        lidar_vert_anchors = 16\n",
    "        lidar_horz_anchors = 30\n",
    "\n",
    "        self.avgpool_camera_visibility = nn.AdaptiveAvgPool2d((camera_vert_anchors, camera_horz_anchors))\n",
    "        self.avgpool_lidar_visibility = nn.AdaptiveAvgPool2d((lidar_vert_anchors, lidar_horz_anchors))\n",
    "\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            self.encoders = ResNet_Encoder_Visibility(camera_in_channels, lidar_in_channels)\n",
    "\n",
    "            # n_embd (int) - number of expected features in the encoder/decoder inputs, n_head (int) - number of heads in the multiheadattention models\n",
    "            self.transformer1_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.layer1[1].conv2.out_channels, # note: in_channels can also be accessed\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.layer2[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer3_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.layer3[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer4_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.layer4[1].conv2.out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "            \n",
    "            if self.encoders.backbone_camera_visibility.layer4[1].conv2.out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera_visibility = nn.Conv2d(self.encoders.backbone_camera_visibility.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar_visibility = nn.Conv2d(self.encoders.backbone_camera_visibility.layer4[1].conv2.out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera_visibility = nn.Sequential()\n",
    "                self.change_channel_conv_lidar_visibility = nn.Sequential()\n",
    "\n",
    "        elif self.architecture == \"MobileNetV3_ViT\":\n",
    "            self.encoders = MobileNetV3_Encoder_Visibility(camera_in_channels, lidar_in_channels)\n",
    "            \n",
    "            self.transformer1_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.features[6].block[3][0].out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            self.transformer2_visibility = GPT(n_embd=self.encoders.backbone_camera_visibility.features[12][0].out_channels,\n",
    "                                            n_head=n_head,\n",
    "                                            block_exp=block_exp,\n",
    "                                            n_layer=n_layer,\n",
    "                                            camera_vert_anchors=camera_vert_anchors,\n",
    "                                            camera_horz_anchors=camera_horz_anchors,\n",
    "                                            lidar_vert_anchors=lidar_vert_anchors,\n",
    "                                            lidar_horz_anchors=lidar_horz_anchors,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embd_pdrop=embd_pdrop,\n",
    "                                            attn_pdrop=attn_pdrop,\n",
    "                                            resid_pdrop=resid_pdrop)\n",
    "\n",
    "            if self.encoders.backbone_camera_visibility.features[12][0].out_channels != perception_output_features:\n",
    "                self.change_channel_conv_camera_visibility = nn.Conv2d(self.encoders.backbone_camera_visibility.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "                self.change_channel_conv_lidar_visibility = nn.Conv2d(self.encoders.backbone_camera_visibility.features[12][0].out_channels, perception_output_features, (1, 1))\n",
    "            else:\n",
    "                self.change_channel_conv_camera_visibility = nn.Sequential()\n",
    "                self.change_channel_conv_lidar_visibility = nn.Sequential()\n",
    "                \n",
    "        else:\n",
    "            assert False, \"Invalid architecture.\"\n",
    "\n",
    "        # classification head        \n",
    "        self.head_visibility = nn.Sequential(nn.Conv2d(perception_output_features, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(p=0.25),\n",
    "                                            nn.Conv2d(128, num_classes_visibility, kernel_size=(1, 1), stride=(1, 1)))\n",
    "\n",
    "    def forward(self, in_camera, in_lidar):\n",
    "        if self.architecture == \"ResNet_ViT\":\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.conv1(in_camera)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.bn1(camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.relu(camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.maxpool(camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.conv1(in_lidar)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.bn1(lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.relu(lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.maxpool(lidar_features_visibility)\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.layer1(camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.layer1(lidar_features_visibility)\n",
    "            camera_features_layer1_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_layer1_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_layer1_visibility, lidar_features_layer1_visibility = self.transformer1_visibility(camera_features_layer1_visibility, lidar_features_layer1_visibility)\n",
    "            camera_features_layer1_visibility = F.interpolate(camera_features_layer1_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer1_visibility = F.interpolate(lidar_features_layer1_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_layer1_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_layer1_visibility\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.layer2(camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.layer2(lidar_features_visibility)\n",
    "            camera_features_layer2_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_layer2_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_layer2_visibility, lidar_features_layer2_visibility = self.transformer2_visibility(camera_features_layer2_visibility, lidar_features_layer2_visibility)\n",
    "            camera_features_layer2_visibility = F.interpolate(camera_features_layer2_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer2_visibility = F.interpolate(lidar_features_layer2_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_layer2_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_layer2_visibility\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.layer3(camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.layer3(lidar_features_visibility)\n",
    "            camera_features_layer3_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_layer3_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_layer3_visibility, lidar_features_layer3_visibility = self.transformer3_visibility(camera_features_layer3_visibility, lidar_features_layer3_visibility)\n",
    "            camera_features_layer3_visibility = F.interpolate(camera_features_layer3_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer3_visibility = F.interpolate(lidar_features_layer3_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_layer3_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_layer3_visibility\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.layer4(camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.layer4(lidar_features_visibility)\n",
    "            camera_features_layer4_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_layer4_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_layer4_visibility, lidar_features_layer4_visibility = self.transformer4_visibility(camera_features_layer4_visibility, lidar_features_layer4_visibility)\n",
    "            camera_features_layer4_visibility = F.interpolate(camera_features_layer4_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_layer4_visibility = F.interpolate(lidar_features_layer4_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_layer4_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_layer4_visibility\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features_visibility = self.change_channel_conv_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_visibility = self.change_channel_conv_lidar_visibility(lidar_features_visibility)\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.avgpool(camera_features_visibility)\n",
    "            camera_features_visibility = torch.flatten(camera_features_visibility, 1)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.avgpool(lidar_features_visibility)\n",
    "            lidar_features_visibility = torch.flatten(lidar_features_visibility, 1)\n",
    "            fused_features_visibility = camera_features_visibility + lidar_features_visibility\n",
    "        \n",
    "        else:\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[0](in_camera)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[1](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[2](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[3](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[4](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[5](camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[0](in_lidar)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[1](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[2](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[3](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[4](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[5](lidar_features_visibility)\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[6](camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[6](lidar_features_visibility)\n",
    "            camera_features_block1_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_block1_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_block1_visibility, lidar_features_block1_visibility = self.transformer1_visibility(camera_features_block1_visibility, lidar_features_block1_visibility)\n",
    "            camera_features_block1_visibility = F.interpolate(camera_features_block1_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block1_visibility = F.interpolate(lidar_features_block1_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_block1_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_block1_visibility\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[7](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[8](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[9](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[10](camera_features_visibility)\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[11](camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[7](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[8](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[9](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[10](lidar_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[11](lidar_features_visibility)\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.features[12](camera_features_visibility)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.features[12](lidar_features_visibility)\n",
    "            camera_features_block2_visibility = self.avgpool_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_block2_visibility = self.avgpool_lidar_visibility(lidar_features_visibility)\n",
    "            camera_features_block2_visibility, lidar_features_block2_visibility = self.transformer2_visibility(camera_features_block2_visibility, lidar_features_block2_visibility)\n",
    "            camera_features_block2_visibility = F.interpolate(camera_features_block2_visibility, size=(camera_features_visibility.shape[2], camera_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            lidar_features_block2_visibility = F.interpolate(lidar_features_block2_visibility, size=(lidar_features_visibility.shape[2], lidar_features_visibility.shape[3]), mode='bilinear', align_corners=False)\n",
    "            camera_features_visibility = camera_features_visibility + camera_features_block2_visibility\n",
    "            lidar_features_visibility = lidar_features_visibility + lidar_features_block2_visibility\n",
    "\n",
    "            # Downsamples channels to 512 (if necessary)\n",
    "            camera_features_visibility = self.change_channel_conv_camera_visibility(camera_features_visibility)\n",
    "            lidar_features_visibility = self.change_channel_conv_lidar_visibility(lidar_features_visibility)\n",
    "\n",
    "            camera_features_visibility = self.encoders.backbone_camera_visibility.avgpool(camera_features_visibility)\n",
    "            camera_features_visibility = torch.flatten(camera_features_visibility, 1)\n",
    "            lidar_features_visibility = self.encoders.backbone_lidar_visibility.avgpool(lidar_features_visibility)\n",
    "            lidar_features_visibility = torch.flatten(lidar_features_visibility, 1)\n",
    "            fused_features_visibility = camera_features_visibility + lidar_features_visibility\n",
    "\n",
    "        # reshape the output to (batch_size, n_features, 1, 1)\n",
    "        fused_features_visibility = fused_features_visibility.view(fused_features_visibility.size(0), -1, 1, 1)\n",
    "        # forward pass through the visibility head\n",
    "        out_visibility = self.head_visibility(fused_features_visibility).view(fused_features_visibility.size(0), -1)\n",
    "        \n",
    "        return out_visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8e29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # useful when there is a large class imbalance. alpha (Tensor, optional): weights for each class. Defaults to None. gamma (float, optional): a constant. Defaults to 0.\n",
    "    # reduction (str, optional): 'mean', 'sum', or 'none', ignore_index (int, optional): class label to ignore. Defaults to -100\n",
    "    def __init__(self, alpha=None, gamma=0.0, reduction='mean', ignore_index=-100):\n",
    "        super().__init__()\n",
    "\n",
    "        assert reduction in ('mean', 'sum', 'none')\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eps = 0.001 # avoid grad explode\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=self.alpha, reduction='none', ignore_index=self.ignore_index)\n",
    "\n",
    "    def forward(self, pred_prob, target):\n",
    "        if pred_prob.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dk) -> (N * d1 * ... * dk, C)\n",
    "            c = pred_prob.shape[1]\n",
    "            pred_prob = pred_prob.permute(0, *range(2, pred_prob.ndim), 1).reshape(-1, c)\n",
    "            target = target.view(-1)\n",
    "\n",
    "        unignored_mask = target != self.ignore_index\n",
    "        target = target[unignored_mask]\n",
    "        if len(target) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        pred_prob = pred_prob[unignored_mask]\n",
    "        \n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = torch.log(pred_prob + self.eps)\n",
    "        ce = self.nll_loss(log_p, target)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(pred_prob))\n",
    "        log_pt = log_p[all_rows, target]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "027c9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "ViT_FusionNet_Weather                              --\n",
      "├─AdaptiveAvgPool2d: 1-1                           --\n",
      "├─AdaptiveAvgPool2d: 1-2                           --\n",
      "├─ResNet_Encoder_Weather: 1-3                      --\n",
      "│    └─ResNet: 2-1                                 --\n",
      "│    │    └─Conv2d: 3-1                            3,136\n",
      "│    │    └─BatchNorm2d: 3-2                       128\n",
      "│    │    └─ReLU: 3-3                              --\n",
      "│    │    └─MaxPool2d: 3-4                         --\n",
      "│    │    └─Sequential: 3-5                        221,952\n",
      "│    │    └─Sequential: 3-6                        1,116,416\n",
      "│    │    └─Sequential: 3-7                        6,822,400\n",
      "│    │    └─Sequential: 3-8                        13,114,368\n",
      "│    │    └─AdaptiveAvgPool2d: 3-9                 --\n",
      "│    │    └─Identity: 3-10                         --\n",
      "│    └─ResNet: 2-2                                 --\n",
      "│    │    └─Conv2d: 3-11                           6,272\n",
      "│    │    └─BatchNorm2d: 3-12                      128\n",
      "│    │    └─ReLU: 3-13                             --\n",
      "│    │    └─MaxPool2d: 3-14                        --\n",
      "│    │    └─Sequential: 3-15                       147,968\n",
      "│    │    └─Sequential: 3-16                       525,568\n",
      "│    │    └─Sequential: 3-17                       2,099,712\n",
      "│    │    └─Sequential: 3-18                       8,393,728\n",
      "│    │    └─AdaptiveAvgPool2d: 3-19                --\n",
      "│    │    └─Identity: 3-20                         --\n",
      "├─GPT: 1-4                                         61,440\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─Sequential: 2-4                             --\n",
      "│    │    └─TransformerBlock: 3-21                 49,984\n",
      "│    │    └─TransformerBlock: 3-22                 49,984\n",
      "│    │    └─TransformerBlock: 3-23                 49,984\n",
      "│    │    └─TransformerBlock: 3-24                 49,984\n",
      "│    │    └─TransformerBlock: 3-25                 49,984\n",
      "│    │    └─TransformerBlock: 3-26                 49,984\n",
      "│    │    └─TransformerBlock: 3-27                 49,984\n",
      "│    │    └─TransformerBlock: 3-28                 49,984\n",
      "│    └─LayerNorm: 2-5                              128\n",
      "├─GPT: 1-5                                         122,880\n",
      "│    └─Dropout: 2-6                                --\n",
      "│    └─Sequential: 2-7                             --\n",
      "│    │    └─TransformerBlock: 3-29                 198,272\n",
      "│    │    └─TransformerBlock: 3-30                 198,272\n",
      "│    │    └─TransformerBlock: 3-31                 198,272\n",
      "│    │    └─TransformerBlock: 3-32                 198,272\n",
      "│    │    └─TransformerBlock: 3-33                 198,272\n",
      "│    │    └─TransformerBlock: 3-34                 198,272\n",
      "│    │    └─TransformerBlock: 3-35                 198,272\n",
      "│    │    └─TransformerBlock: 3-36                 198,272\n",
      "│    └─LayerNorm: 2-8                              256\n",
      "├─GPT: 1-6                                         245,760\n",
      "│    └─Dropout: 2-9                                --\n",
      "│    └─Sequential: 2-10                            --\n",
      "│    │    └─TransformerBlock: 3-37                 789,760\n",
      "│    │    └─TransformerBlock: 3-38                 789,760\n",
      "│    │    └─TransformerBlock: 3-39                 789,760\n",
      "│    │    └─TransformerBlock: 3-40                 789,760\n",
      "│    │    └─TransformerBlock: 3-41                 789,760\n",
      "│    │    └─TransformerBlock: 3-42                 789,760\n",
      "│    │    └─TransformerBlock: 3-43                 789,760\n",
      "│    │    └─TransformerBlock: 3-44                 789,760\n",
      "│    └─LayerNorm: 2-11                             512\n",
      "├─GPT: 1-7                                         491,520\n",
      "│    └─Dropout: 2-12                               --\n",
      "│    └─Sequential: 2-13                            --\n",
      "│    │    └─TransformerBlock: 3-45                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-46                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-47                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-48                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-49                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-50                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-51                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-52                 3,152,384\n",
      "│    └─LayerNorm: 2-14                             1,024\n",
      "├─Sequential: 1-8                                  --\n",
      "├─Sequential: 1-9                                  --\n",
      "├─Sequential: 1-10                                 --\n",
      "│    └─Conv2d: 2-15                                65,664\n",
      "│    └─ReLU: 2-16                                  --\n",
      "│    └─Dropout: 2-17                               --\n",
      "│    └─Conv2d: 2-18                                258\n",
      "===========================================================================\n",
      "Total params: 66,964,418\n",
      "Trainable params: 66,964,418\n",
      "Non-trainable params: 0\n",
      "=========================================================================== ===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "ViT_FusionNet_Visibility                           --\n",
      "├─AdaptiveAvgPool2d: 1-1                           --\n",
      "├─AdaptiveAvgPool2d: 1-2                           --\n",
      "├─ResNet_Encoder_Visibility: 1-3                   --\n",
      "│    └─ResNet: 2-1                                 --\n",
      "│    │    └─Conv2d: 3-1                            3,136\n",
      "│    │    └─BatchNorm2d: 3-2                       128\n",
      "│    │    └─ReLU: 3-3                              --\n",
      "│    │    └─MaxPool2d: 3-4                         --\n",
      "│    │    └─Sequential: 3-5                        221,952\n",
      "│    │    └─Sequential: 3-6                        1,116,416\n",
      "│    │    └─Sequential: 3-7                        6,822,400\n",
      "│    │    └─Sequential: 3-8                        13,114,368\n",
      "│    │    └─AdaptiveAvgPool2d: 3-9                 --\n",
      "│    │    └─Identity: 3-10                         --\n",
      "│    └─ResNet: 2-2                                 --\n",
      "│    │    └─Conv2d: 3-11                           6,272\n",
      "│    │    └─BatchNorm2d: 3-12                      128\n",
      "│    │    └─ReLU: 3-13                             --\n",
      "│    │    └─MaxPool2d: 3-14                        --\n",
      "│    │    └─Sequential: 3-15                       147,968\n",
      "│    │    └─Sequential: 3-16                       525,568\n",
      "│    │    └─Sequential: 3-17                       2,099,712\n",
      "│    │    └─Sequential: 3-18                       8,393,728\n",
      "│    │    └─AdaptiveAvgPool2d: 3-19                --\n",
      "│    │    └─Identity: 3-20                         --\n",
      "├─GPT: 1-4                                         61,440\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─Sequential: 2-4                             --\n",
      "│    │    └─TransformerBlock: 3-21                 49,984\n",
      "│    │    └─TransformerBlock: 3-22                 49,984\n",
      "│    │    └─TransformerBlock: 3-23                 49,984\n",
      "│    │    └─TransformerBlock: 3-24                 49,984\n",
      "│    │    └─TransformerBlock: 3-25                 49,984\n",
      "│    │    └─TransformerBlock: 3-26                 49,984\n",
      "│    │    └─TransformerBlock: 3-27                 49,984\n",
      "│    │    └─TransformerBlock: 3-28                 49,984\n",
      "│    └─LayerNorm: 2-5                              128\n",
      "├─GPT: 1-5                                         122,880\n",
      "│    └─Dropout: 2-6                                --\n",
      "│    └─Sequential: 2-7                             --\n",
      "│    │    └─TransformerBlock: 3-29                 198,272\n",
      "│    │    └─TransformerBlock: 3-30                 198,272\n",
      "│    │    └─TransformerBlock: 3-31                 198,272\n",
      "│    │    └─TransformerBlock: 3-32                 198,272\n",
      "│    │    └─TransformerBlock: 3-33                 198,272\n",
      "│    │    └─TransformerBlock: 3-34                 198,272\n",
      "│    │    └─TransformerBlock: 3-35                 198,272\n",
      "│    │    └─TransformerBlock: 3-36                 198,272\n",
      "│    └─LayerNorm: 2-8                              256\n",
      "├─GPT: 1-6                                         245,760\n",
      "│    └─Dropout: 2-9                                --\n",
      "│    └─Sequential: 2-10                            --\n",
      "│    │    └─TransformerBlock: 3-37                 789,760\n",
      "│    │    └─TransformerBlock: 3-38                 789,760\n",
      "│    │    └─TransformerBlock: 3-39                 789,760\n",
      "│    │    └─TransformerBlock: 3-40                 789,760\n",
      "│    │    └─TransformerBlock: 3-41                 789,760\n",
      "│    │    └─TransformerBlock: 3-42                 789,760\n",
      "│    │    └─TransformerBlock: 3-43                 789,760\n",
      "│    │    └─TransformerBlock: 3-44                 789,760\n",
      "│    └─LayerNorm: 2-11                             512\n",
      "├─GPT: 1-7                                         491,520\n",
      "│    └─Dropout: 2-12                               --\n",
      "│    └─Sequential: 2-13                            --\n",
      "│    │    └─TransformerBlock: 3-45                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-46                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-47                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-48                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-49                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-50                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-51                 3,152,384\n",
      "│    │    └─TransformerBlock: 3-52                 3,152,384\n",
      "│    └─LayerNorm: 2-14                             1,024\n",
      "├─Sequential: 1-8                                  --\n",
      "├─Sequential: 1-9                                  --\n",
      "├─Sequential: 1-10                                 --\n",
      "│    └─Conv2d: 2-15                                65,664\n",
      "│    └─ReLU: 2-16                                  --\n",
      "│    └─Dropout: 2-17                               --\n",
      "│    └─Conv2d: 2-18                                16,512\n",
      "│    └─ReLU: 2-19                                  --\n",
      "│    └─Dropout: 2-20                               --\n",
      "│    └─Conv2d: 2-21                                258\n",
      "===========================================================================\n",
      "Total params: 66,980,930\n",
      "Trainable params: 66,980,930\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_function_weather = FocalLoss(alpha=None, gamma=2.0, reduction='none', ignore_index=-100).to(device)\n",
    "loss_function_visibility = getattr(ordinal_losses, 'OrdinalEncoding')(K=N_CLASSES_VISIBILITY).to(device)\n",
    "n_outputs_visibility = loss_function_visibility.how_many_outputs()\n",
    "\n",
    "# network\n",
    "if NETWORK == \"MobileNetV3_ViT\":\n",
    "    net_weather = ViT_FusionNet_Weather(num_classes_weather=N_CLASSES_WEATHER, architecture=\"MobileNetV3_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=2, block_exp=2, n_layer=4).to(device)\n",
    "    opt_weather = AdamW(net_weather.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    net_visibility = ViT_FusionNet_Visibility(num_classes_visibility=n_outputs_visibility, architecture=\"MobileNetV3_ViT\", camera_in_channels=1, lidar_in_channels=2, n_head=2, block_exp=2, n_layer=4).to(device)\n",
    "    opt_visibility = AdamW(net_visibility.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "elif NETWORK == \"ResNet_ViT\":\n",
    "    net_weather = ViT_FusionNet_Weather(num_classes_weather=N_CLASSES_WEATHER, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2).to(device)\n",
    "    opt_weather = AdamW(net_weather.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    net_visibility = ViT_FusionNet_Visibility(num_classes_visibility=n_outputs_visibility, architecture=\"ResNet_ViT\", camera_in_channels=1, lidar_in_channels=2).to(device)\n",
    "    opt_visibility = AdamW(net_visibility.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "elif NETWORK == \"MobileNetV3_Early\":\n",
    "    net_weather = Mobile_Net_v3_Custom_Weather(3, N_CLASSES_WEATHER).to(device)\n",
    "    opt_weather = AdamW(net_weather.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    net_visibility = Mobile_Net_v3_Custom_Visibility(3, n_outputs_visibility).to(device)\n",
    "    opt_visibility = AdamW(net_visibility.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "else:\n",
    "    assert False, \"Invalid option. Valid options: MobileNetV3_ViT, ResNet_ViT, and MobileNetV3_Early.\"\n",
    "\n",
    "print(summary(net_weather), summary(net_visibility))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a52a0190-97b5-456f-a2db-1161bef2763e",
   "metadata": {},
   "source": [
    "## Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185238c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # training and validation cycles\n",
    "    print(\"[INFO] Network training and validation...\")\n",
    "    PATIENCE = int(0.8*EPOCHS)\n",
    "    vl_weather_loss_min = 1e6\n",
    "    vl_visibility_loss_min = 1e6\n",
    "    wait = 0\n",
    "    loss_avg_weather_tr = []\n",
    "    loss_avg_visibility_tr = []\n",
    "    loss_avg_weather_vl = []\n",
    "    loss_avg_visibility_vl = []\n",
    "\n",
    "    # loop over EPOCHS\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'* Epoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "        loss_total_weather_tr = 0\n",
    "        loss_total_visibility_tr = 0\n",
    "        loss_total_weather_vl = 0\n",
    "        loss_total_visibility_vl = 0\n",
    "\n",
    "        tic = time()\n",
    "        net_weather.train()\n",
    "        net_visibility.train()\n",
    "\n",
    "        for weather_gt_tr, visibility_gt_tr, camera_image_tr, lidar_image_range_tr, lidar_image_intensity_tr in tr: \n",
    "            weather_gt_tr = weather_gt_tr.to(device)\n",
    "            visibility_gt_tr = visibility_gt_tr.to(device)\n",
    "            camera_image_tr = camera_image_tr.to(device)\n",
    "            lidar_image_range_tr = lidar_image_range_tr.to(device)\n",
    "            lidar_image_intensity_tr = lidar_image_intensity_tr.to(device)\n",
    "\n",
    "            if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "                camera_image_tr = camera_image_tr[:,np.newaxis,:,:].float()\n",
    "                lidar_image_tr = torch.cat((lidar_image_range_tr[:,np.newaxis,:,:], lidar_image_intensity_tr[:,np.newaxis,:,:]), dim=1).float()\n",
    "                logits_weather_tr = net_weather(camera_image_tr, lidar_image_tr)\n",
    "                logits_visibility_tr = net_visibility(camera_image_tr, lidar_image_tr)\n",
    "            else:\n",
    "                data_tr = torch.cat((camera_image_tr[:,np.newaxis,:,:], lidar_image_range_tr[:,np.newaxis,:,:], lidar_image_intensity_tr[:,np.newaxis,:,:]), dim=1).float()\n",
    "                logits_weather_tr = net_weather(data_tr)\n",
    "                logits_visibility_tr = net_visibility(data_tr)\n",
    "            \n",
    "            proba_weather_tr = torch.nn.functional.softmax(logits_weather_tr, dim=1)\n",
    "\n",
    "            # forward\n",
    "            loss_weather_tr = loss_function_weather(proba_weather_tr, weather_gt_tr).mean()\n",
    "            loss_visibility_tr = loss_function_visibility(logits_visibility_tr, visibility_gt_tr).mean()\n",
    "            loss_total_weather_tr += loss_weather_tr.item()\n",
    "            loss_total_visibility_tr += loss_visibility_tr.item()\n",
    "\n",
    "            # backward\n",
    "            opt_weather.zero_grad()\n",
    "            loss_weather_tr.backward()\n",
    "            opt_weather.step()\n",
    "            opt_visibility.zero_grad()\n",
    "            loss_visibility_tr.backward()\n",
    "            opt_visibility.step()\n",
    "            \n",
    "        toc = time()\n",
    "        print(f'  Elapsed training time: {toc-tic}s')\n",
    "\n",
    "        tic = time()\n",
    "        net_weather.eval()\n",
    "        net_visibility.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for weather_gt_vl, visibility_gt_vl, camera_image_vl, lidar_image_range_vl, lidar_image_intensity_vl in vl:\n",
    "                weather_gt_vl = weather_gt_vl.to(device)\n",
    "                visibility_gt_vl = visibility_gt_vl.to(device)\n",
    "                camera_image_vl = camera_image_vl.to(device)\n",
    "                lidar_image_range_vl = lidar_image_range_vl.to(device)\n",
    "                lidar_image_intensity_vl = lidar_image_intensity_vl.to(device)\n",
    "\n",
    "                if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "                    camera_image_vl = camera_image_vl[:,np.newaxis,:,:].float()\n",
    "                    lidar_image_vl = torch.cat((lidar_image_range_vl[:,np.newaxis,:,:], lidar_image_intensity_vl[:,np.newaxis,:,:]), dim=1).float()\n",
    "                    logits_weather_vl = net_weather(camera_image_vl, lidar_image_vl)\n",
    "                    logits_visibility_vl = net_visibility(camera_image_vl, lidar_image_vl)\n",
    "                else:\n",
    "                    data_vl = torch.cat((camera_image_vl[:,np.newaxis,:,:], lidar_image_range_vl[:,np.newaxis,:,:], lidar_image_intensity_vl[:,np.newaxis,:,:]), dim=1).float()\n",
    "                    logits_weather_vl = net_weather(data_vl)\n",
    "                    logits_visibility_vl = net_visibility(data_vl)\n",
    "                \n",
    "                proba_weather_vl = torch.nn.functional.softmax(logits_weather_vl, dim=1)\n",
    "\n",
    "                # forward\n",
    "                loss_weather_vl = loss_function_weather(proba_weather_vl, weather_gt_vl).mean()\n",
    "                loss_visibility_vl = loss_function_visibility(logits_visibility_vl, visibility_gt_vl).mean()\n",
    "                loss_total_weather_vl += loss_weather_vl.item() \n",
    "                loss_total_visibility_vl += loss_visibility_vl.item()\n",
    "\n",
    "        toc = time()\n",
    "\n",
    "        loss_avg_weather_tr.append(loss_total_weather_tr / len(tr))\n",
    "        loss_avg_visibility_tr.append(loss_total_visibility_tr / len(tr))\n",
    "        loss_avg_weather_vl.append(loss_total_weather_vl / len(vl))\n",
    "        loss_avg_visibility_vl.append(loss_total_visibility_vl / len(vl))\n",
    "\n",
    "        print(f'  Elapsed validation time: {toc-tic}s')\n",
    "        print(f'  Tr Weather Loss: {loss_avg_weather_tr[epoch]}, Tr Visibility Loss: {loss_avg_visibility_tr[epoch]}, Vl Weather Loss: {loss_avg_weather_vl[epoch]}, Vl Visibility Loss: {loss_avg_visibility_vl[epoch]}')\n",
    "\n",
    "        # save models if validation loss has decreased\n",
    "        if loss_avg_weather_vl[epoch] <= vl_weather_loss_min:\n",
    "            print(f'  The best weather model was saved!')\n",
    "            torch.save(net_weather, path_best_weather_model)\n",
    "            vl_weather_loss_min = loss_avg_weather_vl[epoch]\n",
    "            wait = 0\n",
    "        # early stopping\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                print(f\"Terminated training for early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if loss_avg_visibility_vl[epoch] <= vl_visibility_loss_min:\n",
    "            print(f'  The best visibility model was saved!')\n",
    "            torch.save(net_visibility, path_best_visibility_model)\n",
    "            vl_visibility_loss_min = loss_avg_visibility_vl[epoch]\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                print(f\"Terminated training for early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    print(f'The last models were saved!')\n",
    "    torch.save(net_weather, path_last_weather_model)\n",
    "    torch.save(net_visibility, path_last_visibility_model)\n",
    "\n",
    "    # plot loss\n",
    "    epochs_plot = range(1, (len(loss_avg_weather_vl)+1))\n",
    "    plt.plot(epochs_plot, loss_avg_weather_tr)\n",
    "    plt.plot(epochs_plot, loss_avg_visibility_tr)\n",
    "    plt.plot(epochs_plot, loss_avg_weather_vl)\n",
    "    plt.plot(epochs_plot, loss_avg_visibility_vl)\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(epochs_plot)\n",
    "    plt.legend(('Tr Weather Loss', 'Tr Visibility Loss', 'Vl Weather Loss', 'Vl Visibility Loss'), loc='upper right')\n",
    "    plt.savefig('ST_MM_W_MOR_Class_Train_Val_Loss_Network_{}_Seed_{}.pdf'.format(NETWORK, SEED))\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16939702-a980-437f-9816-36b3a37fa337",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e331aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and analyze model memory footprint\n",
    "a = torch.cuda.memory_allocated(device)\n",
    "saved_weather_model = torch.load(path_best_weather_model, map_location=torch.device(device))\n",
    "saved_visibility_model = torch.load(path_best_visibility_model, map_location=torch.device(device))\n",
    "b = torch.cuda.memory_allocated(device)\n",
    "\n",
    "print(\"Are the models on cuda: \", next(saved_weather_model.parameters()).is_cuda, next(saved_visibility_model.parameters()).is_cuda)\n",
    "model_memory = (b - a)/(1024**2)\n",
    "print(\"Total memory of the models:\", model_memory, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the mean inference time ######\n",
    "camera_image_dummy = torch.randn(1, 1, 512, 960).to(device)\n",
    "lidar_image_dummy = torch.randn(1, 2, 512, 960).to(device)\n",
    "data_image_dummy = torch.cat((camera_image_dummy, lidar_image_dummy), dim=1)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 10000\n",
    "timings = np.zeros((repetitions, 1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # gpu-warm-up\n",
    "    for _ in range(100):\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            _ = saved_weather_model(camera_image_dummy, lidar_image_dummy)\n",
    "            _ = saved_visibility_model(camera_image_dummy, lidar_image_dummy)\n",
    "        else:\n",
    "            _ = saved_weather_model(data_image_dummy)\n",
    "            _ = saved_visibility_model(data_image_dummy)\n",
    "    # measure performance\n",
    "    for rep in range(repetitions):\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            starter.record()\n",
    "            _ = saved_weather_model(camera_image_dummy, lidar_image_dummy)\n",
    "            _ = saved_visibility_model(camera_image_dummy, lidar_image_dummy)\n",
    "            ender.record()\n",
    "        else:\n",
    "            starter.record()\n",
    "            _ = saved_weather_model(data_image_dummy)\n",
    "            _ = saved_visibility_model(data_image_dummy)\n",
    "            ender.record()\n",
    "        # wait for GPU sync\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Testing the network...\")\n",
    "saved_weather_model.eval() # set model to evaluation mode\n",
    "saved_visibility_model.eval()\n",
    "\n",
    "metrics_weather = [torchmetrics.classification.MulticlassAccuracy(num_classes=N_CLASSES_WEATHER, average='weighted').to(device), torchmetrics.CohenKappa(task='multiclass', num_classes=N_CLASSES_WEATHER).to(device), torchmetrics.F1Score(task='multiclass', num_classes=N_CLASSES_WEATHER, average='weighted').to(device)]\n",
    "cm_weather = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=N_CLASSES_WEATHER, normalize='none').to(device)\n",
    "\n",
    "metrics_visibility = [torchmetrics.classification.MulticlassAccuracy(num_classes=N_CLASSES_VISIBILITY, average='weighted').to(device), torchmetrics.CohenKappa(task='multiclass', num_classes=N_CLASSES_VISIBILITY).to(device), torchmetrics.F1Score(task='multiclass', num_classes=N_CLASSES_VISIBILITY, average='weighted').to(device)]\n",
    "cm_visibility = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=N_CLASSES_VISIBILITY, normalize='none').to(device)\n",
    "\n",
    "tic = time()\n",
    "with torch.no_grad(): # turn off gradient tracking\n",
    "    for weather_gt_ts, visibility_gt_ts, camera_image_ts, lidar_image_range_ts, lidar_image_intensity_ts in ts:\n",
    "        weather_gt_ts = weather_gt_ts.to(device)\n",
    "        visibility_gt_ts = visibility_gt_ts.to(device)\n",
    "        camera_image_ts = camera_image_ts.to(device)\n",
    "        lidar_image_range_ts = lidar_image_range_ts.to(device)\n",
    "        lidar_image_intensity_ts = lidar_image_intensity_ts.to(device)\n",
    "\n",
    "        if NETWORK == \"MobileNetV3_ViT\" or NETWORK == \"ResNet_ViT\":\n",
    "            camera_image_ts = camera_image_ts[:,np.newaxis,:,:].float()\n",
    "            lidar_image_ts = torch.cat((lidar_image_range_ts[:,np.newaxis,:,:], lidar_image_intensity_ts[:,np.newaxis,:,:]), dim=1).float()\n",
    "            logits_weather_ts = saved_weather_model(camera_image_ts, lidar_image_ts)\n",
    "            logits_visibility_ts = saved_visibility_model(camera_image_ts, lidar_image_ts)\n",
    "        else:\n",
    "            data_ts = torch.cat((camera_image_ts[:,np.newaxis,:,:], lidar_image_range_ts[:,np.newaxis,:,:], lidar_image_intensity_ts[:,np.newaxis,:,:]), dim=1).float()\n",
    "            logits_weather_ts = saved_weather_model(data_ts)\n",
    "            logits_visibility_ts = saved_visibility_model(data_ts)\n",
    "   \n",
    "        proba_weather_ts = torch.nn.functional.softmax(logits_weather_ts, dim=1)\n",
    "        class_weather_ts = torch.argmax(proba_weather_ts, dim=1)\n",
    "\n",
    "        class_visibility_ts = loss_function_visibility.to_classes(logits_visibility_ts)\n",
    "\n",
    "        for metric in metrics_weather:\n",
    "            metric.update(class_weather_ts, weather_gt_ts)\n",
    "        cm_weather.update(class_weather_ts, weather_gt_ts)\n",
    "\n",
    "        for metric in metrics_visibility:\n",
    "            metric.update(class_visibility_ts, visibility_gt_ts)\n",
    "        cm_visibility.update(class_visibility_ts, visibility_gt_ts)\n",
    "\n",
    "toc = time()\n",
    "print(f'Elapsed test time: {toc-tic}s')\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=2.0, w_pad=1.0, h_pad=5.0)\n",
    "sns.heatmap(cm_weather.compute().cpu(), ax=axs[0], annot=True, fmt='g')\n",
    "sns.heatmap(cm_visibility.compute().cpu(), ax=axs[1], annot=True, fmt='g')\n",
    "axs[0].set_title('Weather Classification')\n",
    "axs[0].set_xlabel('Predicted Label')\n",
    "axs[0].set_ylabel('True Label')\n",
    "axs[0].xaxis.set_ticklabels(['Fog', 'Rain'])\n",
    "axs[0].yaxis.set_ticklabels(['Fog', 'Rain'])\n",
    "axs[1].set_title('MOR Classification')\n",
    "axs[1].set_xlabel('Predicted Label')\n",
    "axs[1].set_ylabel('True Label')\n",
    "axs[1].xaxis.set_ticklabels(['0-40', '40-200', '>200'])\n",
    "axs[1].yaxis.set_ticklabels(['0-40', '40-200', '>200'])\n",
    "plt.savefig('ST_MM_W_MOR_Class_Confusion_Matrix_Network_{}_Seed_{}.pdf'.format(NETWORK, SEED))\n",
    "plt.close()\n",
    "\n",
    "# save metrics\n",
    "with open('ST_MM_W_MOR_Class_Metrics_Network_{}_Seed_{}.csv'.format(NETWORK, SEED),'w') as f:\n",
    "    writer = csv.writer(f, dialect='excel')\n",
    "    writer.writerow([\"Metric\", \"Value\"])\n",
    "    for metric in metrics_weather:\n",
    "        writer.writerow([\"Weather\" + str(metric.__class__.__name__), metric.compute().item()])\n",
    "\n",
    "    for metric in metrics_visibility:\n",
    "        writer.writerow([\"Visibility\" + str(metric.__class__.__name__), metric.compute().item()])\n",
    "\n",
    "    writer.writerow([\"ModelMemory\", model_memory])\n",
    "    writer.writerow([\"MeanInferenceTime\", mean_syn])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
